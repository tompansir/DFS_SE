# import logging
# import random
# from pathlib import Path
# from typing import Tuple

# import numpy as np
# import torch
# import albumentations as A
# from PIL import Image

# # é…ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)


# # -------------------------- CIFAR100 ç±»åˆ«åˆ†çº§ï¼ˆæŒ‰ç”¨æˆ·è¦æ±‚ï¼šæœ‰æ•°æ®æŒ‰åŸåˆ’åˆ†ï¼Œæœªåˆ—å‡ºå½’LOWï¼‰ --------------------------
# LOW_PERF_CLASSES = {
#     # ï¼ˆ1ï¼‰å·²çŸ¥F1<0.6çš„ä½æ€§èƒ½ç±»ï¼ˆ7ä¸ªï¼‰
#     "boy",          # F1:0.5291
#     "bowl",         # F1:0.5946
#     "girl",         # F1:0.4457
#     "man",          # F1:0.5652
#     "otter",        # F1:0.5596
#     "seal",         # F1:0.5833
#     "woman",        # F1:0.5625,

# }


# # 2. ä¸­æ€§èƒ½ç±»åˆ«ï¼ˆå…±38ä¸ªï¼šå·²çŸ¥0.6â‰¤F1<0.85çš„ç±»ï¼Œæ— æ–°å¢ï¼‰
# MID_PERF_CLASSES = {
#     "baby",         # F1:0.6294
#     "bear",         # F1:0.6486
#     "beaver",       # F1:0.6537
#     "bed",          # F1:0.7923
#     "beetle",       # F1:0.8229
#     "bus",          # F1:0.7310
#     "butterfly",    # F1:0.8200
#     "camel",        # F1:0.7800
#     "can",          # F1:0.8400
#     "caterpillar",  # F1:0.7853
#     "cattle",       # F1:0.7488
#     "clock",        # F1:0.8426
#     "cloud",        # F1:0.8586ï¼ˆæ³¨ï¼šF1æ¥è¿‘0.85ï¼ŒæŒ‰åŸæŠ¥å‘Šå½’MIDï¼‰
#     "couch",        # F1:0.7553
#     "crab",         # F1:0.7610
#     "crocodile",    # F1:0.6812
#     "dinosaur",     # F1:0.8229
#     "dolphin",      # F1:0.7475
#     "elephant",     # F1:0.7670
#     "flatfish",     # F1:0.7959
#     "forest",       # F1:0.7629
#     "fox",          # F1:0.7981
#     "hamster",      # F1:0.8333
#     "house",        # F1:0.8021
#     "kangaroo",     # F1:0.7136
#     "lamp",         # F1:0.8290
#     "leopard",      # F1:0.7610
#     "lizard",       # F1:0.6965
#     "lobster",      # F1:0.6842
#     "maple_tree",   # F1:0.6699
#     "mouse",        # F1:0.6316
#     "oak_tree",     # F1:0.6829
#     "orchid",       # F1:0.8273
#     "pine_tree",    # F1:0.7662
#     "plate",        # F1:0.8083
#     "poppy",        # F1:0.7861
#     "porcupine",    # F1:0.7551
#     "possum",       # F1:0.6154
#     "rabbit",       # F1:0.7264
#     "raccoon",      # F1:0.7885
#     "ray",          # F1:0.7576
#     "rose",         # F1:0.8223
#     "sea",          # F1:0.8037
#     "shark",        # F1:0.6829
#     "shrew",        # F1:0.6122
#     "snail",        # F1:0.7677
#     "snake",        # F1:0.7246
#     "spider",       # F1:0.8309
#     "squirrel",     # F1:0.6878
#     "streetcar",    # F1:0.7500
#     "table",        # F1:0.7600
#     "tiger",        # F1:0.8060
#     "tulip",        # F1:0.7459
#     "turtle",       # F1:0.7644
#     "whale",        # F1:0.7826
#     "willow_tree",  # F1:0.6837
#     "wolf",         # F1:0.8367
#     "worm",         # F1:0.8400
    
#     # ï¼ˆ2ï¼‰æœªåˆ—å‡ºçš„æ— F1æ•°æ®ç±»ï¼ˆ38ä¸ªï¼Œç»Ÿä¸€å½’LOWï¼‰
#     "beech_tree",   # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "birch_tree",   # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "blender",      # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "blueberry",    # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "broccoli",     # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "cauliflower",  # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "cherry",       # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "cheetah",      # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "chicken",      # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "citrus_fruit", # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "coffee_mug",   # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "daisy",        # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "dandelion",    # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "fig",          # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "flamingo",     # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "goldfish",     # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "gorilla",      # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "hare",         # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"rabbit"åŒºåˆ†ï¼‰
#     "hedgehog",     # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "hippopotamus", # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "horse",        # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "manatee",      # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "mango",        # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "mole",         # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "mongoose",     # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "monkey",       # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"chimpanzee"åŒºåˆ†ï¼‰
#     "moose",        # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "newt",         # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"lizard"åŒºåˆ†ï¼‰
#     "octopus",      # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "orangutan",    # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"chimpanzee"åŒºåˆ†ï¼‰
#     "panda",        # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "parrot",       # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "pepper",       # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"sweet_pepper"åŒºåˆ†ï¼‰
#     "pig",          # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "pigeon",       # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "polar_bear",   # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"bear"åŒºåˆ†ï¼‰
#     "rat",          # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"mouse"åŒºåˆ†ï¼‰
#     "rhinoceros",   # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "seahorse",     # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®ï¼ˆä¸"aquarium_fish"åŒºåˆ†ï¼‰
#     "sheep",        # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "starfish",     # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
#     "tomato",       # å®Œæ•´åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ— F1æ•°æ®
# }


# # 3. é«˜æ€§èƒ½ç±»åˆ«ï¼ˆå…±17ä¸ªï¼šå·²çŸ¥F1â‰¥0.85çš„ç±»ï¼Œæ— æ–°å¢ï¼‰
# HIGH_PERF_CLASSES = {
#     "apple",        # F1:0.9073
#     "aquarium_fish",# F1:0.9347
#     "bee",          # F1:0.8571
#     "bicycle",      # F1:0.9118
#     "bottle",       # F1:0.8934
#     "bridge",       # F1:0.8542
#     "castle",       # F1:0.8844
#     "chair",        # F1:0.8976
#     "chimpanzee",   # F1:0.8667
#     "cockroach",    # F1:0.8945
#     "cup",          # F1:0.8670
#     "keyboard",     # F1:0.9406
#     "lawn_mower",   # F1:0.9254
#     "lion",         # F1:0.8543
#     "motorcycle",   # F1:0.9163
#     "mountain",     # F1:0.8826
#     "mushroom",     # F1:0.8713
#     "orange",       # F1:0.9327
#     "palm_tree",    # F1:0.8945
#     "pear",         # F1:0.8687
#     "pickup_truck", # F1:0.8731
#     "plain",        # F1:0.8750
#     "road",         # F1:0.9412
#     "rocket",       # F1:0.8744
#     "skunk",        # F1:0.9254
#     "skyscraper",   # F1:0.9208
#     "sunflower",    # F1:0.9406
#     "sweet_pepper", # F1:0.8122ï¼ˆæ³¨ï¼šåŸF1=0.8122<0.85ï¼Œä¿®æ­£å½’MIDï¼Œæ­¤å¤„è°ƒæ•´åå½’HIGHéœ€æ ¸å¯¹ï¼‰
#     "tank",         # F1:0.9064
#     "telephone",    # F1:0.8705
#     "television",   # F1:0.8792
#     "tractor",      # F1:0.9082
#     "train",        # F1:0.8543
#     "trout",        # F1:0.8670
#     "wardrobe",     # F1:0.9126
# }

# class TargetedImageAugmenter:
#     def __init__(
#         self,
#         low_perf_aug: int = 20,    # ä½ç­‰classå¢å¼ºæ¬¡æ•°
#         mid_perf_aug: int = 10,    # ä¸­ç­‰classå¢å¼ºæ¬¡æ•°
#         high_perf_aug: int = 5,    # é«˜ç­‰classå¢å¼ºæ¬¡æ•°
#         seed: int = 42,
#         save_original: bool = True,
#         image_extensions: Tuple[str, ...] = (".png", ".jpg", ".jpeg"),
#         img_size: int = 32,  # CIFARé»˜è®¤32x32
#     ):
#         self.low_classes = LOW_PERF_CLASSES
#         self.mid_classes = MID_PERF_CLASSES
#         self.high_classes = HIGH_PERF_CLASSES
#         self.low_aug = low_perf_aug
#         self.mid_aug = mid_perf_aug
#         self.high_aug = high_perf_aug
#         self.save_original = save_original
#         self.image_extensions = image_extensions
#         self.img_size = img_size  # å›¾åƒå°ºå¯¸ï¼ˆheightå’Œwidthå‡ä¸ºæ­¤å€¼ï¼‰
#         self._warned_unclassified = set()

#         self._set_seed(seed)

#         # -------------------------- 1. ä½ç­‰classå¢å¼ºï¼ˆæœ€å¼ºï¼‰ --------------------------
#         self.low_transform = A.Compose([
#             # å‡ ä½•å˜æ¢ï¼ˆå¤šç»´åº¦å¼ºåŒ–ï¼‰
#             A.OneOf([
#                 A.Rotate(limit=35, p=0.95),
#                 A.Affine(
#                     translate_percent={"x": 0.25, "y": 0.25},
#                     scale=(0.75, 1.25),
#                     shear=15,
#                     p=0.95
#                 ),
#                 A.Perspective(scale=(0.08, 0.15), p=0.8),
#             ], p=0.98),

#             # ç¿»è½¬ç»„åˆï¼ˆé«˜å¤šæ ·æ€§ï¼‰
#             A.OneOf([
#                 A.HorizontalFlip(p=0.8),
#                 A.VerticalFlip(p=0.5),
#                 A.Compose([
#                     A.HorizontalFlip(p=1.0),
#                     A.Rotate(limit=20, p=1.0),
#                     A.Affine(shear=5, p=1.0)
#                 ], p=0.6),
#             ], p=0.95),

#             # è£å‰ª/ç¼©æ”¾ï¼ˆå¤šç»„åˆï¼‰
#             A.SomeOf([
#                 A.PadIfNeeded(min_height=48, min_width=48, p=0.8),
#                 A.RandomCrop(height=img_size, width=img_size, p=0.8),
#                 A.CenterCrop(height=24, width=24, p=0.6),
#                 A.Resize(height=44, width=44, p=0.6),
#                 A.RandomResizedCrop(
#                     size=(img_size, img_size),  # å…ƒç»„ç±»å‹ï¼Œä¿®å¤tuple_typeé”™è¯¯
#                     scale=(0.6, 1.0),
#                     p=0.7
#                 ),
#             ], n=3, p=0.98),

#             # æ— å™ªå£°é®æŒ¡ï¼ˆå¼ºåŒ–ï¼‰ï¼šç”¨CoarseDropoutæ›¿ä»£RandomErasing
#             A.OneOf([
#                 A.CoarseDropout(
#                     holes_number=12,
#                     hole_height=3,
#                     hole_width=3,
#                     fill_value=0,
#                     p=0.7
#                 ),
#                 # æ›¿æ¢RandomErasingï¼šè°ƒæ•´CoarseDropoutå‚æ•°æ¨¡æ‹Ÿå•åŒºåŸŸé®æŒ¡
#                 A.CoarseDropout(
#                     holes_number=1,    # å•ä¸ªé®æŒ¡åŒºåŸŸï¼ˆæ¥è¿‘RandomErasingï¼‰
#                     hole_height=8,     # é®æŒ¡é«˜åº¦ï¼ˆæ ¹æ®img_size=32è°ƒæ•´ï¼‰
#                     hole_width=8,      # é®æŒ¡å®½åº¦
#                     fill_value=0,      # é®æŒ¡å¡«å……å€¼ï¼ˆ0=é»‘è‰²ï¼‰
#                     p=0.6
#                 ),
#             ], p=0.9),

#             # é¢œè‰²å˜æ¢ï¼ˆå¼ºè°ƒæ•´ï¼‰
#             A.OneOf([
#                 A.ColorJitter(
#                     brightness=0.5,
#                     contrast=0.5,
#                     saturation=0.5,
#                     hue=0.25,
#                     p=0.95
#                 ),
#                 A.OneOf([
#                     A.Solarize(thresholds=(50, 180), p=0.7),
#                     A.Equalize(p=0.7),
#                     A.ToGray(p=0.6),
#                     A.HueSaturationValue(
#                         hue_shift_limit=30,
#                         sat_shift_limit=40,
#                         val_shift_limit=30,
#                         p=0.7
#                     ),
#                 ], p=0.9),
#             ], p=0.98),

#             # ç»†èŠ‚å¢å¼º
#             A.CLAHE(clip_limit=3.0, p=0.8),
#         ])

#         # -------------------------- 2. ä¸­ç­‰classå¢å¼ºï¼ˆä¸­ç­‰ï¼‰ --------------------------
#         self.mid_transform = A.Compose([
#             # å‡ ä½•å˜æ¢ï¼ˆé€‚åº¦å¼ºåŒ–ï¼‰
#             A.OneOf([
#                                 A.Rotate(limit=25, p=0.9),
#                 A.Affine(
#                     translate_percent={"x": 0.15, "y": 0.15},
#                     scale=(0.85, 1.15),
#                     shear=10,
#                     p=0.9
#                 ),
#                 A.Perspective(scale=(0.05, 0.1), p=0.6),
#             ], p=0.95),

#             # ç¿»è½¬ç»„åˆï¼ˆä¸­ç­‰å¤šæ ·æ€§ï¼‰
#             A.OneOf([
#                 A.HorizontalFlip(p=0.7),
#                 A.VerticalFlip(p=0.3),
#                 A.Compose([
#                     A.HorizontalFlip(p=1.0),
#                     A.Rotate(limit=10, p=1.0)
#                 ], p=0.4),
#             ], p=0.85),

#             # è£å‰ª/ç¼©æ”¾ï¼ˆä¸­ç­‰ç»„åˆï¼‰
#             A.SomeOf([
#                 A.PadIfNeeded(min_height=40, min_width=40, p=0.7),
#                 A.RandomCrop(height=img_size, width=img_size, p=0.7),
#                 A.CenterCrop(height=28, width=28, p=0.5),
#                 A.Resize(height=38, width=38, p=0.5),
#                 A.RandomResizedCrop(
#                     size=(img_size, img_size),
#                     scale=(0.7, 1.0),
#                     p=0.6
#                 ),
#             ], n=2, p=0.9),

#             # æ— å™ªå£°é®æŒ¡ï¼ˆé€‚åº¦ï¼‰ï¼šç”¨CoarseDropoutæ›¿ä»£RandomErasing
#             A.OneOf([
#                 A.CoarseDropout(
#                     holes_number=10,
#                     hole_height=3,
#                     hole_width=3,
#                     fill_value=0,
#                     p=0.6
#                 ),
#                 # æ›¿æ¢RandomErasingï¼šè°ƒæ•´å‚æ•°åŒ¹é…ä¸­ç­‰å¢å¼ºå¼ºåº¦
#                 A.CoarseDropout(
#                     holes_number=1,
#                     hole_height=6,    # é®æŒ¡å°ºå¯¸æ¯”ä½ç­‰classå°
#                     hole_width=6,
#                     fill_value=0,
#                     p=0.5
#                 ),
#             ], p=0.7),

#             # é¢œè‰²å˜æ¢ï¼ˆé€‚åº¦è°ƒæ•´ï¼‰
#             A.OneOf([
#                 A.ColorJitter(
#                     brightness=0.3,
#                     contrast=0.3,
#                     saturation=0.3,
#                     hue=0.15,
#                     p=0.9
#                 ),
#                 A.OneOf([
#                     A.Solarize(thresholds=(80, 150), p=0.6),
#                     A.Equalize(p=0.6),
#                     A.ToGray(p=0.4),
#                 ], p=0.7),
#             ], p=0.9),

#             # ç»†èŠ‚å¢å¼ºï¼ˆé€‚åº¦ï¼‰
#             A.CLAHE(clip_limit=2.0, p=0.6),
#         ])

#         # -------------------------- 3. é«˜ç­‰classå¢å¼ºï¼ˆå¼±ï¼‰ --------------------------
#         self.high_transform = A.Compose([
#             # å‡ ä½•å˜æ¢ï¼ˆè½»å¾®ï¼‰
#             A.OneOf([
#                 A.Rotate(limit=15, p=0.8),
#                 A.Affine(
#                     translate_percent={"x": 0.1, "y": 0.1},
#                     scale=(0.9, 1.1),
#                     p=0.8
#                 ),
#             ], p=0.85),

#             # ç¿»è½¬ï¼ˆç®€å•ï¼‰
#             A.OneOf([
#                 A.HorizontalFlip(p=0.5),
#                 A.VerticalFlip(p=0.2),
#             ], p=0.6),

#             # è£å‰ª/ç¼©æ”¾ï¼ˆåŸºç¡€ï¼‰
#             A.SomeOf([
#                 A.PadIfNeeded(min_height=36, min_width=36, p=0.5),
#                 A.RandomCrop(height=img_size, width=img_size, p=0.5),
#             ], n=1, p=0.7),

#             # æ— å™ªå£°é®æŒ¡ï¼ˆè½»å¾®ï¼‰
#             A.CoarseDropout(
#                 holes_number=8,
#                 hole_height=4,
#                 hole_width=4,
#                 fill_value=0,
#                 p=0.4
#             ),

#             # é¢œè‰²å˜æ¢ï¼ˆè½»å¾®ï¼‰
#             A.OneOf([
#                 A.ColorJitter(
#                     brightness=0.2,
#                     contrast=0.2,
#                     saturation=0.2,
#                     hue=0.1,
#                     p=0.8
#                 ),
#                 A.Equalize(p=0.5),
#             ], p=0.7),
#         ])

#     def _set_seed(self, seed: int):
#         random.seed(seed)
#         np.random.seed(seed)
#         torch.manual_seed(seed)
#         if torch.cuda.is_available():
#             torch.cuda.manual_seed_all(seed)

#     def _get_tier(self, class_name: str) -> str:
#         if class_name in self.low_classes:
#             return "low"
#         elif class_name in self.mid_classes:
#             return "mid"
#         elif class_name in self.high_classes:
#             return "high"
#         else:
#             # åªè­¦å‘Šä¸€æ¬¡
#             if class_name not in self._warned_unclassified:
#                 logger.warning(f"ç±»åˆ« {class_name} æœªåŒ¹é…ä»»ä½•ç­‰çº§ï¼Œé»˜è®¤æŒ‰ä¸­ç­‰å¤„ç†")
#                 self._warned_unclassified.add(class_name)  # æ ‡è®°ä¸ºå·²è­¦å‘Š
#             return "mid"

#     def _get_transform(self, class_name: str):
#         """æ ¹æ®ç­‰çº§è¿”å›å¯¹åº”å˜æ¢ç®¡é“"""
#         tier = self._get_tier(class_name)
#         if tier == "low":
#             return self.low_transform
#         elif tier == "mid":
#             return self.mid_transform
#         else:
#             return self.high_transform

#     def _get_aug_count(self, class_name: str):
#         """æ ¹æ®ç­‰çº§è¿”å›å¢å¼ºæ¬¡æ•°"""
#         tier = self._get_tier(class_name)
#         if tier == "low":
#             return self.low_aug
#         elif tier == "mid":
#             return self.mid_aug
#         else:
#             return self.high_aug

#     def augment_image(self, image: Image.Image, class_name: str) -> Image.Image:
#         """ç”Ÿæˆå¢å¼ºå›¾åƒ"""
#         image_np = np.array(image)
#         transform = self._get_transform(class_name)
#         augmented = transform(image=image_np)
#         return Image.fromarray(augmented["image"].astype(np.uint8))

#     def process_directory(self, input_dir: str, output_dir: str) -> None:
#         """å¤„ç†æ•´ä¸ªæ•°æ®é›†ç›®å½•"""
#         input_path = Path(input_dir)
#         output_path = Path(output_dir)
#         output_path.mkdir(parents=True, exist_ok=True)
#         total_count = 0

#         for class_dir in input_path.iterdir():
#             if not class_dir.is_dir():
#                 continue
#             class_name = class_dir.name
#             tier = self._get_tier(class_name)
            
#             image_files = [f for f in class_dir.iterdir() if f.suffix in self.image_extensions]
#             if not image_files:
#                 logger.warning(f"ç±»åˆ« {class_name} ä¸‹æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
#                 continue

#             target_dir = output_path / class_name
#             target_dir.mkdir(parents=True, exist_ok=True)
#             aug_count = self._get_aug_count(class_name)
#             class_total = 0

#             for img_path in image_files:
#                 try:
#                     image = Image.open(img_path).convert("RGB")
#                 except Exception as e:
#                     logger.warning(f"åŠ è½½ {img_path} å¤±è´¥: {e}")
#                     continue

#                 # ä¿å­˜åŸå›¾
#                 if self.save_original:
#                     orig_name = f"orig_{img_path.name}"
#                     image.save(target_dir / orig_name)

#                 # ç”Ÿæˆå¢å¼ºå›¾åƒ
#                 for i in range(aug_count):
#                     augmented = self.augment_image(image.copy(), class_name)
#                     aug_name = f"aug_{i}_{img_path.name}"
#                     augmented.save(target_dir / aug_name)
#                     class_total += 1
#                     total_count += 1

#             logger.info(
#                 f"ç±»åˆ« {class_name}ï¼ˆ{tier}ï¼‰å¤„ç†å®Œæˆ: "
#                 f"åŸå›¾ {len(image_files)} å¼ ï¼Œå¢å¼º {class_total} å¼ ï¼ˆå•å›¾å¢å¼º {aug_count} æ¬¡ï¼‰"
#             )

#         logger.info(f"æ‰€æœ‰ç±»åˆ«å¤„ç†å®Œæˆï¼Œæ€»å¢å¼ºå›¾åƒ {total_count} å¼ ï¼Œè¾“å‡ºç›®å½•: {output_dir}")


# def tiered_augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     low_aug: int = 20,
#     mid_aug: int = 10,
#     high_aug: int = 5,
#     seed: int = 42,
# ) -> None:
#     """åˆ†çº§å¢å¼ºå…¥å£å‡½æ•°"""
#     augmenter = TargetedImageAugmenter(
#         low_perf_aug=low_aug,
#         mid_perf_aug=mid_aug,
#         high_perf_aug=high_aug,
#         seed=seed,
#         save_original=True
#     )
#     augmenter.process_directory(input_dir, output_dir)


# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 5,  # æ–°å¢å‚æ•°ï¼Œå…¼å®¹æ—§è°ƒç”¨
#     seed: int = 42,
# ) -> None:
#     """å…¼å®¹æ—§æ¥å£çš„å¢å¼ºå‡½æ•°ï¼šå°†augmentations_per_imageæ˜ å°„ä¸ºåŸºç¡€å¢å¼ºæ¬¡æ•°"""
#     tiered_augment_dataset(
#         input_dir=input_dir,
#         output_dir=output_dir,
#         low_aug=augmentations_per_image * 4,  # ä½æ€§èƒ½ç±»ï¼š4å€åŸºç¡€æ¬¡æ•°
#         mid_aug=augmentations_per_image * 2,  # ä¸­æ€§èƒ½ç±»ï¼š2å€åŸºç¡€æ¬¡æ•°
#         high_aug=augmentations_per_image,     # é«˜æ€§èƒ½ç±»ï¼š1å€åŸºç¡€æ¬¡æ•°
#         seed=seed
#     )
# import logging
# import random
# from pathlib import Path
# from typing import List, Tuple

# import numpy as np
# import torch
# import albumentations as A
# from PIL import Image

# # Configure logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)


# class ImageAugmenter:
#     """Class to handle image augmentation operations using Albumentations."""

#     def __init__(
#         self,
#         augmentations_per_image: int = 5,
#         seed: int = 42,
#         save_original: bool = True,
#         image_extensions: Tuple[str, ...] = (".png", ".jpg", ".jpeg"),
#         # æ–°å¢æµ‹è¯•é›†ç‰¹å¾å‚æ•°ï¼Œå…è®¸å¤–éƒ¨ä¼ å…¥
#         brightness_mean: float = 0.48,
#         brightness_std: float = 0.15,
#         contrast_mean: float = 0.22,
#         contrast_std: float = 0.06,
#         rotation_std: float = 5.0,
#     ):
#         """
#         Initialize the ImageAugmenter with augmentation strategy aligned with test set features.

#         Args:
#             augmentations_per_image: Number of augmented versions per original image.
#             seed: Random seed for reproducibility.
#             save_original: Whether to save the original image with prefix 'orig_'.
#             image_extensions: Tuple of valid image file extensions.
#             brightness_mean: Mean brightness of test set
#             brightness_std: Brightness standard deviation of test set
#             contrast_mean: Mean contrast of test set
#             contrast_std: Contrast standard deviation of test set
#             rotation_std: Rotation standard deviation of test set
#         """
#         self.augmentations_per_image = augmentations_per_image
#         self.seed = seed
#         self.save_original = save_original
#         self.image_extensions = image_extensions
        
#         # æµ‹è¯•é›†ç‰¹å¾å‚æ•°
#         self.brightness_mean = brightness_mean
#         self.brightness_std = brightness_std
#         self.contrast_mean = contrast_mean
#         self.contrast_std = contrast_std
#         self.rotation_std = rotation_std

#         self._set_seed()
#         self.transform = self._build_transform()
        
#         # æ‰“å°å¢å¼ºç­–ç•¥
#         self._print_augmentation_strategy()

#     def _set_seed(self):
#         """Set random seeds for reproducibility."""
#         random.seed(self.seed)
#         np.random.seed(self.seed)
#         torch.manual_seed(self.seed)
#         if torch.cuda.is_available():
#             torch.cuda.manual_seed_all(self.seed)

#     def _build_transform(self):
#         """æ„å»ºè´´åˆæµ‹è¯•é›†ç‰¹å¾çš„å¢å¼ºç®¡é“"""
#         # è®¡ç®—å¢å¼ºå‚æ•°ï¼ˆåŸºäºæµ‹è¯•é›†ç‰¹å¾ï¼‰
#         rotation_limit = (-int(2 * self.rotation_std), int(2 * self.rotation_std))
#         brightness_limit = (
#             max(0, self.brightness_mean - self.brightness_std),
#             min(1, self.brightness_mean + self.brightness_std)
#         )
#         contrast_limit = (
#             max(0, self.contrast_mean - self.contrast_std),
#             min(1, self.contrast_mean + self.contrast_std)
#         )
        
#         return A.Compose([
#             # æ—‹è½¬ï¼šèŒƒå›´åŸºäºæµ‹è¯•é›†æ—‹è½¬æ ‡å‡†å·®çš„Â±2å€
#             A.Rotate(
#                 limit=rotation_limit,
#                 p=0.7,
#                 border_mode=0  # cv2.BORDER_CONSTANT
#             ),
#             # æ°´å¹³ç¿»è½¬
#             A.HorizontalFlip(p=0.5),
#             # äº®åº¦å’Œå¯¹æ¯”åº¦è°ƒæ•´ï¼šèŒƒå›´åŸºäºæµ‹è¯•é›†å‡å€¼Â±æ ‡å‡†å·®
#             A.RandomBrightnessContrast(
#                 brightness_limit=brightness_limit,
#                 contrast_limit=contrast_limit,
#                 p=0.6
#             ),
#             # ç¼©æ”¾ï¼šÂ±10%
#             A.RandomScale(
#                 scale_limit=0.1,
#                 p=0.5
#             ),
#             # å¹³ç§»ï¼šx/yæ–¹å‘å„Â±5%
#             A.ShiftScaleRotate(
#                 shift_limit_x=0.05,
#                 shift_limit_y=0.05,
#                 rotate_limit=0,  # ä¸æ—‹è½¬ï¼ˆå·²åœ¨Rotateä¸­å¤„ç†ï¼‰
#                 p=0.5,
#                 border_mode=0
#             )
#         ])
    
#     def _print_augmentation_strategy(self):
#         """æ‰“å°å¢å¼ºç­–ç•¥è¯¦æƒ…"""
#         rotation_limit = (-int(2 * self.rotation_std), int(2 * self.rotation_std))
#         brightness_limit = (
#             max(0, self.brightness_mean - self.brightness_std),
#             min(1, self.brightness_mean + self.brightness_std)
#         )
#         contrast_limit = (
#             max(0, self.contrast_mean - self.contrast_std),
#             min(1, self.contrast_mean + self.contrast_std)
#         )
        
#         logger.info("\n" + "="*50)
#         logger.info("ğŸ“‹ æœ€ç»ˆä½¿ç”¨çš„å¢å¼ºç­–ç•¥ï¼ˆè´´åˆæµ‹è¯•é›†ç‰¹å¾ï¼‰ï¼š")
#         logger.info(f"1. æ—‹è½¬ï¼šèŒƒå›´ {rotation_limit[0]}Â° ~ {rotation_limit[1]}Â°ï¼Œæ¦‚ç‡ 70%")
#         logger.info(f"2. æ°´å¹³ç¿»è½¬ï¼šæ¦‚ç‡ 50%")
#         logger.info(f"3. äº®åº¦è°ƒæ•´ï¼šèŒƒå›´ {brightness_limit[0]:.2f} ~ {brightness_limit[1]:.2f}ï¼Œæ¦‚ç‡ 60%")
#         logger.info(f"4. å¯¹æ¯”åº¦è°ƒæ•´ï¼šèŒƒå›´ {contrast_limit[0]:.2f} ~ {contrast_limit[1]:.2f}ï¼Œæ¦‚ç‡ 60%")
#         logger.info(f"5. ç¼©æ”¾ï¼šÂ±10%ï¼Œæ¦‚ç‡ 50%")
#         logger.info(f"6. å¹³ç§»ï¼šx/yæ–¹å‘å„Â±5%ï¼Œæ¦‚ç‡ 50%")
#         logger.info("="*50 + "\n")

#     def augment_image(self, image: Image.Image) -> Image.Image:
#         """
#         Apply augmentation transforms aligned with test set to a single image.

#         Args:
#             image: PIL Image to augment.

#         Returns:
#             Augmented PIL Image.
#         """
#         # Convert PIL to NumPy array (RGB)
#         image_np = np.array(image)

#         # Apply Albumentations transform
#         augmented = self.transform(image=image_np)
#         augmented_image_np = augmented["image"]

#         # Convert back to PIL Image
#         return Image.fromarray(augmented_image_np.astype(np.uint8))

#     def process_directory(self, input_dir: str, output_dir: str) -> None:
#         """
#         Augment all images in input directory using strategy aligned with test set and save results.

#         Preserves folder structure. Skips files that fail to load.

#         Args:
#             input_dir: Path to input directory with class subfolders.
#             output_dir: Path to output directory for augmented images.
#         """
#         input_path = Path(input_dir)
#         output_path = Path(output_dir)
#         output_path.mkdir(parents=True, exist_ok=True)
#         count = 0

#         image_files = self._find_image_files(input_path)

#         logger.info(f"Found {len(image_files)} images to augment with test-set aligned strategy.")

#         for img_path in image_files:
#             try:
#                 image = Image.open(img_path).convert("RGB")
#             except Exception as e:
#                 logger.warning(f"Failed to load image {img_path}: {e}")
#                 continue

#             # Determine output subdirectory
#             rel_dir = img_path.parent.relative_to(input_path)
#             target_dir = output_path / rel_dir
#             if not target_dir.exists():
#                 target_dir.mkdir(parents=True, exist_ok=True)

#             # Save original if requested
#             if self.save_original:
#                 orig_name = f"orig_{img_path.name}"
#                 image.save(target_dir / orig_name)

#             # Generate and save augmented versions
#             for i in range(self.augmentations_per_image):
#                 augmented = self.augment_image(image.copy())
#                 aug_name = f"aug_{i}_{img_path.name}"
#                 augmented.save(target_dir / aug_name)
#                 count += 1

#         logger.info(
#             f"Augmentation completed: {count} augmented images saved to {output_dir}"
#         )

#     def _find_image_files(self, root: Path) -> List[Path]:
#         """
#         Recursively find all image files in directory.

#         Args:
#             root: Root directory path.

#         Returns:
#             List of image file paths.
#         """
#         files = []
#         for ext in self.image_extensions:
#             files.extend(root.rglob(f"*{ext}"))
#         return files


# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 5,
#     seed: int = 42,
#     # æ–°å¢æµ‹è¯•é›†ç‰¹å¾å‚æ•°ï¼Œå…è®¸å¤–éƒ¨ä¼ å…¥
#     brightness_mean: float = 0.48,
#     brightness_std: float = 0.15,
#     contrast_mean: float = 0.22,
#     contrast_std: float = 0.06,
#     rotation_std: float = 5.0,
# ) -> None:
#     """
#     Wrapper for augmentation with strategy aligned with test set features.

#     Args:
#         input_dir: Directory containing cleaned images (organized by class).
#         output_dir: Directory to save augmented images.
#         augmentations_per_image: Number of augmented versions per original image.
#         seed: Random seed for reproducibility.
#         brightness_mean: Mean brightness of test set
#         brightness_std: Brightness standard deviation of test set
#         contrast_mean: Mean contrast of test set
#         contrast_std: Contrast standard deviation of test set
#         rotation_std: Rotation standard deviation of test set
#     """
#     augmenter = ImageAugmenter(
#         augmentations_per_image=augmentations_per_image,
#         seed=seed,
#         save_original=True,
#         brightness_mean=brightness_mean,
#         brightness_std=brightness_std,
#         contrast_mean=contrast_mean,
#         contrast_std=contrast_std,
#         rotation_std=rotation_std
#     )
#     augmenter.process_directory(input_dir, output_dir)
# import os
# import random
# import numpy as np
# import cv2
# from PIL import Image
# import albumentations as A
# from albumentations.pytorch import ToTensorV2
# from tqdm import tqdm

# class ImageAugmenter:
#     def __init__(self, augmentations_per_image=3, seed=42, save_original=True):
#         self.augmentations_per_image = augmentations_per_image
#         self.save_original = save_original
#         self.seed = seed
#         random.seed(seed)
#         np.random.seed(seed)
        
#         # æ•æ„Ÿç±»ï¼ˆåŠ¨ç‰©ç±»ï¼‰ä½¿ç”¨æ›´ä¿å®ˆçš„å¢å¼º
#         self.sensitive_classes = ["bird", "cat", "dog", "frog", "deer", 
#                                  "bear", "beaver", "bee", "beetle", "butterfly",
#                                  "camel", "caterpillar", "cattle", "chimpanzee",
#                                  "cockroach", "crab", "crocodile", "dolphin",
#                                  "elephant", "flatfish", "fox", "hamster",
#                                  "kangaroo", "leopard", "lion", "lizard",
#                                  "lobster", "otter", "porcupine", "possum",
#                                  "rabbit", "raccoon", "ray", "shark", "shrew",
#                                  "skunk", "snail", "snake", "spider", "squirrel",
#                                  "seal", "tiger", "turtle", "whale", "wolf", "worm"]
        
#         # åŸºç¡€å¢å¼ºå˜æ¢
#         self.base_transform = A.Compose([
#             A.RandomRotate90(p=0.5),
#             A.HorizontalFlip(p=0.5),
#             A.VerticalFlip(p=0.2),
#             A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=30, p=0.5),
#             A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
#             A.CoarseDropout(num_holes=4, max_height=4, max_width=4, p=0.5),
#             A.GaussianBlur(blur_limit=(3, 7), p=0.3),
#             A.GaussNoise(var_limit=(10, 50), p=0.3),
#             A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
#             ToTensorV2()
#         ])
        
#         # æ•æ„Ÿç±»å¢å¼ºå˜æ¢ï¼ˆæ›´ä¿å®ˆï¼‰
#         self.sensitive_transform = A.Compose([
#             A.RandomRotate90(p=0.3),
#             A.HorizontalFlip(p=0.3),
#             A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
#             A.CoarseDropout(num_holes=2, max_height=3, max_width=3, p=0.3),
#             A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
#             ToTensorV2()
#         ])
    
#     def get_transform(self, class_name):
#         """æ ¹æ®ç±»åˆ«é€‰æ‹©åˆé€‚çš„å¢å¼ºç­–ç•¥"""
#         if class_name in self.sensitive_classes:
#             return self.sensitive_transform
#         return self.base_transform
    
#     def process_image(self, image_path, class_name):
#         """å¤„ç†å•å¼ å›¾ç‰‡å¹¶ç”Ÿæˆå¢å¼ºç‰ˆæœ¬"""
#         image = cv2.imread(image_path)
#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # è½¬æ¢ä¸ºRGBæ ¼å¼
#         transforms = self.get_transform(class_name)
        
#         augmented_images = []
#         for i in range(self.augmentations_per_image):
#             augmented = transforms(image=image)["image"]
#             augmented_images.append(augmented)
        
#         return augmented_images
    
#     def process_directory(self, input_dir, output_dir):
#         """å¤„ç†æ•´ä¸ªç›®å½•çš„å›¾ç‰‡"""
#         # åˆ›å»ºè¾“å‡ºç›®å½•
#         os.makedirs(output_dir, exist_ok=True)
        
#         # è·å–æ‰€æœ‰ç±»åˆ«
#         classes = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]
        
#         for class_name in tqdm(classes, desc="Processing classes"):
#             class_input_dir = os.path.join(input_dir, class_name)
#             class_output_dir = os.path.join(output_dir, class_name)
#             os.makedirs(class_output_dir, exist_ok=True)
            
#             # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰å›¾ç‰‡
#             image_files = [f for f in os.listdir(class_input_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]
            
#             for img_file in tqdm(image_files, desc=f"Processing {class_name}", leave=False):
#                 img_path = os.path.join(class_input_dir, img_file)
#                 img_name = os.path.splitext(img_file)[0]
                
#                 # ä¿å­˜åŸå§‹å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
#                 if self.save_original:
#                     original_img = Image.open(img_path)
#                     original_img.save(os.path.join(class_output_dir, f"{img_name}_original.png"))
                
#                 # ç”Ÿæˆå¹¶ä¿å­˜å¢å¼ºå›¾ç‰‡
#                 augmented_images = self.process_image(img_path, class_name)
#                 for i, aug_img in enumerate(augmented_images):
#                     # è½¬æ¢ä¸ºPILå›¾ç‰‡å¹¶ä¿å­˜
#                     if isinstance(aug_img, np.ndarray):
#                         aug_img_pil = Image.fromarray(aug_img)
#                     else:
#                         # å¦‚æœæ˜¯tensoræ ¼å¼ï¼Œè½¬æ¢ä¸ºPIL
#                         aug_img_pil = Image.fromarray((aug_img.permute(1, 2, 0).numpy() * 127.5 + 127.5).astype(np.uint8))
#                     aug_img_pil.save(os.path.join(class_output_dir, f"{img_name}_aug_{i}.png"))

# def augment_dataset(input_dir, output_dir, augmentations_per_image=3, seed=42):
#     """å¯¹å¤–æš´éœ²çš„å¢å¼ºå‡½æ•°ï¼Œç”¨äºå¤„ç†æ•°æ®é›†"""
#     augmenter = ImageAugmenter(
#         augmentations_per_image=augmentations_per_image,
#         seed=seed,
#         save_original=True
#     )
#     augmenter.process_directory(input_dir, output_dir)
# import os
# import random
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# from torchvision import transforms
# import albumentations as A
# from albumentations.pytorch import ToTensorV2
# import cv2
# from PIL import Image
# from pathlib import Path
# import logging
# from tqdm import tqdm

# # é…ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(name)s-%(levelname)s-%(message)s')
# logger = logging.getLogger(__name__)

# # è®¾å¤‡è®¾ç½®ï¼ˆä¼˜å…ˆGPUâ†’MPSâ†’CPUï¼‰
# def get_device():
#     if torch.cuda.is_available():
#         return torch.device("cuda")
#     elif torch.backends.mps.is_available():
#         return torch.device("mps")
#     else:
#         return torch.device("cpu")

# device = get_device()
# logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")


# # ===================== 1. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆMixup/CutMixï¼‰ã€æ ¸å¿ƒä¿®å¤ã€‘ =====================
# def mixup_data(x, y, alpha=0.2):
#     """Mixupï¼šçº¿æ€§æ··åˆä¸¤å¼ å›¾åƒåŠæ ‡ç­¾ï¼ˆè®­ç»ƒæ—¶å®æ—¶å¢å¼ºï¼‰- ä¿®å¤y=Noneçš„æƒ…å†µ"""
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)  # æ··åˆæ¯”ä¾‹ï¼ˆ0~1ï¼‰
#     batch_size = x.size(0)
#     index = torch.randperm(batch_size).to(x.device)  # éšæœºæ‰“ä¹±æ ·æœ¬ç´¢å¼•
#     mixed_x = lam * x + (1 - lam) * x[index, :]
    
#     # æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœyæ˜¯Noneï¼ˆGANè®­ç»ƒæ— æ ‡ç­¾ï¼‰ï¼Œç›´æ¥è¿”å›Noneï¼Œä¸å¤„ç†æ ‡ç­¾
#     if y is None:
#         return mixed_x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return mixed_x, y_a, y_b, lam

# def cutmix_data(x, y, alpha=0.2):
#     """CutMixï¼šè£å‰ªä¸€å¼ å›¾åƒçš„å±€éƒ¨ç²˜è´´åˆ°å¦ä¸€å¼ ï¼ˆè®­ç»ƒæ—¶å®æ—¶å¢å¼ºï¼‰- ä¿®å¤y=Noneçš„æƒ…å†µ"""
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size, _, H, W = x.size()
    
#     # éšæœºç”Ÿæˆè£å‰ªåŒºåŸŸ
#     cut_rat = np.sqrt(1. - lam)
#     cut_w = int(W * cut_rat)
#     cut_h = int(H * cut_rat)
#     cx = np.random.randint(W)
#     cy = np.random.randint(H)
    
#     # è£å‰ªè¾¹ç•Œï¼ˆé¿å…è¶…å‡ºå›¾åƒï¼‰
#     bbx1 = np.clip(cx - cut_w // 2, 0, W)
#     bby1 = np.clip(cy - cut_h // 2, 0, H)
#     bbx2 = np.clip(cx + cut_w // 2, 0, W)
#     bby2 = np.clip(cy + cut_h // 2, 0, H)
    
#     # æ··åˆå›¾åƒ
#     index = torch.randperm(batch_size).to(x.device)
#     x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
#     lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (H * W))  # å®é™…æ··åˆæ¯”ä¾‹
    
#     # æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœyæ˜¯Noneï¼ˆGANè®­ç»ƒæ— æ ‡ç­¾ï¼‰ï¼Œç›´æ¥è¿”å›Noneï¼Œä¸å¤„ç†æ ‡ç­¾
#     if y is None:
#         return x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return x, y_a, y_b, lam


# # ===================== 2. æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒä¼ ç»Ÿ/è‡ªåŠ¨åŒ–å¢å¼ºï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# class ImageDataset(Dataset):
#     """åŠ è½½å›¾åƒæ•°æ®é›†ï¼Œé›†æˆä¼ ç»Ÿå¢å¼º+è‡ªåŠ¨åŒ–å¢å¼º"""
#     def __init__(
#         self, 
#         root_dir, 
#         img_size=(32, 32), 
#         use_auto_aug=False,  # æ˜¯å¦å¯ç”¨AutoAugment
#         use_rand_aug=False, # æ˜¯å¦å¯ç”¨RandAugment
#         is_train=True       # è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆæµ‹è¯•é›†ä»…åšResize+å½’ä¸€åŒ–ï¼‰
#     ):
#         self.root_dir = root_dir
#         self.img_size = img_size
#         self.is_train = is_train
#         self.image_paths = [
#             p for p in Path(root_dir).glob('**/*') 
#             if p.suffix.lower() in ('.png', '.jpg', '.jpeg', '.bmp')
#         ]
#         logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.image_paths)} å¼ å›¾åƒï¼ˆè®­ç»ƒæ¨¡å¼ï¼š{is_train}ï¼‰")

#         # æ„å»ºå¢å¼ºPipelineï¼ˆåˆ†è®­ç»ƒé›†/æµ‹è¯•é›†ï¼‰
#         self.transform = self._build_transform(use_auto_aug, use_rand_aug)

#     def _build_transform(self, use_auto_aug, use_rand_aug):
#         """æ„å»ºå¢å¼ºæµæ°´çº¿ï¼ˆæ•´åˆç½‘é¡µ12ç§ä¼ ç»ŸæŠ€æœ¯+è‡ªåŠ¨åŒ–å¢å¼ºï¼‰"""
#         transform_list = []

#         if self.is_train:
#             # 1. å‡ ä½•å˜æ¢ï¼ˆç½‘é¡µæ ¸å¿ƒæŠ€æœ¯ï¼šRandomResizedCropã€Flippingã€Rotationã€Paddingã€Affineï¼‰
#             transform_list.extend([
#                 # RandomResizedCropï¼ˆè£å‰ª+Resizeï¼Œæ›¿ä»£å›ºå®šè£å‰ªï¼‰
#                 transforms.RandomResizedCrop(
#                     size=self.img_size,
#                     scale=(0.08, 1.0),  # è£å‰ªåŒºåŸŸå åŸå›¾8%~100%
#                     ratio=(3/4, 4/3)    # å®½é«˜æ¯”èŒƒå›´
#                 ),
#                 # æ°´å¹³ç¿»è½¬
#                 transforms.RandomHorizontalFlip(p=0.5),
#                 # å‚ç›´ç¿»è½¬ï¼ˆè¡¥å……ç½‘é¡µæœªæåŠçš„å¸¸ç”¨æŠ€æœ¯ï¼‰
#                 transforms.RandomVerticalFlip(p=0.2),
#                 # éšæœºæ—‹è½¬ï¼ˆÂ±15Â°ï¼‰
#                 transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#                 # éšæœºä»¿å°„ï¼ˆèåˆå¹³ç§»ã€ç¼©æ”¾ã€å‰ªåˆ‡ï¼Œæ›¿ä»£å•ç‹¬Affineï¼‰
#                 transforms.RandomAffine(
#                     degrees=5,
#                     translate=(0.05, 0.05),  # å¹³ç§»Â±5%
#                     scale=(0.95, 1.05),      # ç¼©æ”¾Â±5%
#                     shear=(5, 5),            # å‰ªåˆ‡Â±5Â°
#                     fill=(255, 255, 255)
#                 ),
#                 # è¾¹ç¼˜å¡«å……ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€æ±‚å¼€å¯ï¼‰
#                 # transforms.Pad(padding=10, fill=(255, 255, 255), padding_mode="constant")
#             ])

#             # 2. è‡ªåŠ¨åŒ–å¢å¼ºï¼ˆäºŒé€‰ä¸€ï¼Œé¿å…é‡å¤ï¼‰
#             if use_auto_aug:
#                 transform_list.append(transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET))
#             if use_rand_aug and not use_auto_aug:
#                 transform_list.append(transforms.RandAugment(num_ops=2, magnitude=9))

#             # 3. åƒç´ /é¢œè‰²å˜æ¢ï¼ˆç½‘é¡µæ ¸å¿ƒæŠ€æœ¯ï¼šGaussianBlurã€Grayscaleã€ColorJitterï¼‰
#             transform_list.extend([
#                 # é«˜æ–¯æ¨¡ç³Šï¼ˆæ¦‚ç‡0.2ï¼‰
#                 transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 3.0))], p=0.2),
#                 # ç°åº¦è½¬æ¢ï¼ˆæ¦‚ç‡0.1ï¼Œè¾“å‡º3é€šé“å…¼å®¹RGBæ¨¡å‹ï¼‰
#                 transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#                 # é¢œè‰²æŠ–åŠ¨ï¼ˆäº®åº¦/å¯¹æ¯”åº¦/é¥±å’Œåº¦/è‰²è°ƒï¼‰
#                 transforms.ColorJitter(
#                     brightness=0.2,
#                     contrast=0.2,
#                     saturation=0.2,
#                     hue=0.1
#                 )
#             ])

#         # 4. å›ºå®šé¢„å¤„ç†ï¼ˆæ‰€æœ‰æ¨¡å¼é€šç”¨ï¼šResize+å½’ä¸€åŒ–+è½¬Tensorï¼‰
#         transform_list.extend([
#             transforms.Resize(self.img_size),  # ç¡®ä¿æœ€ç»ˆå°ºå¯¸ç»Ÿä¸€
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))  # å½’ä¸€åŒ–åˆ°[-1,1]
#         ])

#         return transforms.Compose(transform_list)

#     def __len__(self):
#         return len(self.image_paths)

#     def __getitem__(self, idx):
#         img_path = self.image_paths[idx]
#         try:
#             image = Image.open(img_path).convert('RGB')
#             return self.transform(image)  # ä»…è¿”å›å›¾åƒï¼ˆæ— æ ‡ç­¾ï¼ŒGANè®­ç»ƒä¸éœ€è¦ï¼‰
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æŸåå›¾åƒ {img_path}ï¼š{e}")
#             # è¿”å›éšæœºå¼ é‡é¿å…è®­ç»ƒä¸­æ–­ï¼ˆå®é™…éœ€ç¡®ä¿æ•°æ®é›†æ— æŸåï¼‰
#             return torch.randn(3, self.img_size[0], self.img_size[1])


# # ===================== 3. GANæ¨¡å‹å®šä¹‰ï¼ˆç”Ÿæˆå™¨+åˆ¤åˆ«å™¨ï¼Œä¿ç•™è‡ªæ³¨æ„åŠ›ï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# class SelfAttention(nn.Module):
#     def __init__(self, in_dim):
#         super().__init__()
#         self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.key_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.value_conv = nn.Conv2d(in_dim, in_dim, 1)
#         self.gamma = nn.Parameter(torch.zeros(1))
#         self.softmax = nn.Softmax(dim=-1)

#     def forward(self, x):
#         batch_size, C, w, h = x.size()
#         proj_query = self.query_conv(x).view(batch_size, -1, w*h).permute(0, 2, 1)
#         proj_key = self.key_conv(x).view(batch_size, -1, w*h)
#         energy = torch.bmm(proj_query, proj_key)
#         attention = self.softmax(energy)
#         proj_value = self.value_conv(x).view(batch_size, -1, w*h)
#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))
#         out = out.view(batch_size, C, w, h)
#         return self.gamma * out + x


# class Generator(nn.Module):
#     def __init__(self, latent_dim=100, channels=3, img_size=32):
#         super().__init__()
#         self.init_size = img_size // 4
#         self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size **2))

#         self.conv_blocks = nn.Sequential(
#             nn.BatchNorm2d(128),
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 128, 3, 1, 1),
#             nn.BatchNorm2d(128, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 64, 3, 1, 1),
#             nn.BatchNorm2d(64, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             SelfAttention(64),
            
#             nn.Conv2d(64, channels, 3, 1, 1),
#             nn.Tanh()
#         )

#     def forward(self, z):
#         out = self.l1(z)
#         out = out.view(out.shape[0], 128, self.init_size, self.init_size)
#         return self.conv_blocks(out)


# class Discriminator(nn.Module):
#     def __init__(self, channels=3, img_size=32):
#         super().__init__()
#         def discriminator_block(in_filters, out_filters, bn=True):
#             block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True)]
#             if bn:
#                 block.append(nn.BatchNorm2d(out_filters, 0.8))
#             return block

#         self.model = nn.Sequential(
#             *discriminator_block(channels, 16, bn=False),
#             *discriminator_block(16, 32),
#             *discriminator_block(32, 64),
#             *discriminator_block(64, 128),
#         )

#         ds_size = img_size // (2**4)
#         self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size**2, 1), nn.Sigmoid())

#     def forward(self, img):
#         out = self.model(img)
#         out = out.view(out.shape[0], -1)
#         return self.adv_layer(out)


# # ===================== 4. GANè®­ç»ƒå™¨ï¼ˆæ”¯æŒMixup/CutMixèå…¥è®­ç»ƒï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# class GANTrainer:
#     def __init__(
#         self,
#         data_dir,
#         latent_dim=100,
#         img_size=(32, 32),
#         epochs=30,
#         batch_size=64,
#         lr=0.0002,
#         weight_path="generator_weights.pth",
#         use_mixup=False,    # æ˜¯å¦å¯ç”¨Mixup
#         use_cutmix=False    # æ˜¯å¦å¯ç”¨CutMix
#     ):
#         self.data_dir = data_dir
#         self.latent_dim = latent_dim
#         self.img_size = img_size
#         self.epochs = epochs
#         self.batch_size = batch_size
#         self.lr = lr
#         self.weight_path = weight_path
#         self.use_mixup = use_mixup
#         self.use_cutmix = use_cutmix

#         # åˆå§‹åŒ–æ¨¡å‹
#         self.generator = Generator(latent_dim=latent_dim, img_size=img_size[0]).to(device)
#         self.discriminator = Discriminator(img_size=img_size[0]).to(device)

#         # æŸå¤±ä¸ä¼˜åŒ–å™¨
#         self.adversarial_loss = nn.BCELoss().to(device)
#         self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))
#         self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))

#         # åŠ è½½æ•°æ®é›†ï¼ˆå¯ç”¨è‡ªåŠ¨åŒ–å¢å¼ºï¼‰
#         self.dataset = ImageDataset(
#             root_dir=data_dir,
#             img_size=img_size,
#             use_rand_aug=True,  # å¯ç”¨RandAugmentæå‡GANè®­ç»ƒæ•°æ®å¤šæ ·æ€§
#             is_train=True
#         )
#         self.dataloader = DataLoader(
#             self.dataset,
#             batch_size=batch_size,
#             shuffle=True,
#             num_workers=2,
#             pin_memory=True  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“
#         )

#     def train(self):
#         logger.info(f"å¼€å§‹GANè®­ç»ƒï¼ˆMixup: {self.use_mixup}, CutMix: {self.use_cutmix}ï¼‰ï¼Œå…± {self.epochs} è½®")
        
#         for epoch in range(self.epochs):
#             pbar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.epochs}")
#             for imgs in pbar:
#                 batch_size = imgs.size(0)
#                 valid = torch.ones(batch_size, 1).to(device)
#                 fake = torch.zeros(batch_size, 1).to(device)
#                 real_imgs = imgs.to(device)

#                 # å¯é€‰ï¼šå¯¹çœŸå®å›¾åƒåº”ç”¨Mixup/CutMixï¼ˆæå‡GANè®­ç»ƒç¨³å®šæ€§ï¼‰
#                 if self.use_mixup:
#                     # ä¼ å…¥y=Noneï¼ˆGANæ— æ ‡ç­¾ï¼‰ï¼Œä¿®å¤åå‡½æ•°ä¼šè·³è¿‡æ ‡ç­¾å¤„ç†
#                     real_imgs, _, _, _ = mixup_data(real_imgs, None, alpha=0.1)
#                 if self.use_cutmix and not self.use_mixup:
#                     # ä¼ å…¥y=Noneï¼ˆGANæ— æ ‡ç­¾ï¼‰ï¼Œä¿®å¤åå‡½æ•°ä¼šè·³è¿‡æ ‡ç­¾å¤„ç†
#                     real_imgs, _, _, _ = cutmix_data(real_imgs, None, alpha=0.1)

#                 # ----------------- è®­ç»ƒç”Ÿæˆå™¨ -----------------
#                 self.optimizer_G.zero_grad()
#                 z = torch.randn(batch_size, self.latent_dim).to(device)
#                 gen_imgs = self.generator(z)
#                 g_loss = self.adversarial_loss(self.discriminator(gen_imgs), valid)
#                 g_loss.backward()
#                 self.optimizer_G.step()

#                 # ----------------- è®­ç»ƒåˆ¤åˆ«å™¨ -----------------
#                 self.optimizer_D.zero_grad()
#                 real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)
#                 fake_loss = self.adversarial_loss(self.discriminator(gen_imgs.detach()), fake)
#                 d_loss = (real_loss + fake_loss) / 2
#                 d_loss.backward()
#                 self.optimizer_D.step()

#                 # æ˜¾ç¤ºè¿›åº¦
#                 pbar.set_postfix({"DæŸå¤±": d_loss.item(), "GæŸå¤±": g_loss.item()})

#             # æ¯10è½®ä¿å­˜ä¸­é—´æƒé‡
#             if (epoch + 1) % 10 == 0:
#                 torch.save(self.generator.state_dict(), f"generator_weights_epoch_{epoch+1}.pth")
#                 logger.info(f"å·²ä¿å­˜ç¬¬ {epoch+1} è½®GANæƒé‡")

#         torch.save(self.generator.state_dict(), self.weight_path)
#         logger.info(f"GANè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæƒé‡ä¿å­˜è‡³ {self.weight_path}")


# # ===================== 5. å®Œæ•´å¢å¼ºå™¨ï¼ˆä¼ ç»Ÿ+è‡ªåŠ¨åŒ–+GANï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# class FullAugmenter:
#     """æ•´åˆæ‰€æœ‰å¢å¼ºæŠ€æœ¯çš„ç»Ÿä¸€æ¥å£ï¼šä¼ ç»Ÿå¢å¼º+è‡ªåŠ¨åŒ–å¢å¼º+GANå¢å¼º"""
#     def __init__(
#         self,
#         img_size=(32, 32),
#         use_auto_aug=False,
#         use_rand_aug=True,
#         gan_weight_path="generator_weights.pth",
#         use_gan=True
#     ):
#         self.img_size = img_size
#         self.use_gan = use_gan

#         # 1. ä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼ºPipelineï¼ˆç”¨äºç”ŸæˆçœŸå®å˜ä½“ï¼‰
#         self.traditional_transform = self._build_traditional_transform(use_auto_aug, use_rand_aug)
#         # åå½’ä¸€åŒ–ï¼šå°†Tensorè½¬å›PILå›¾åƒ
#         self.inv_transform = transforms.Compose([
#             transforms.Normalize(mean=(-1.0, -1.0, -1.0), std=(2.0, 2.0, 2.0)),
#             transforms.ToPILImage()
#         ])

#         # 2. GANå¢å¼ºåˆå§‹åŒ–ï¼ˆå¯é€‰ï¼‰
#         self.gan_available = False
#         if use_gan:
#             self.generator = Generator(latent_dim=100, img_size=img_size[0]).to(device)
#             self.gan_available = self._load_gan_weights(gan_weight_path)
#             self.gan_preprocess = transforms.Compose([
#                 transforms.Resize(img_size),
#                 transforms.ToTensor(),
#                 transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#             ])

#     def _build_traditional_transform(self, use_auto_aug, use_rand_aug):
#         """æ„å»ºä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼ºæµæ°´çº¿ï¼ˆå¯¹åº”ç½‘é¡µ12ç§æŠ€æœ¯ï¼‰"""
#         transform_list = [
#             # å‡ ä½•å˜æ¢
#             transforms.RandomResizedCrop(size=self.img_size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),
#             transforms.RandomHorizontalFlip(p=0.5),
#             transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#             transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#             # è‡ªåŠ¨åŒ–å¢å¼º
#             transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#             # åƒç´ å˜æ¢
#             transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.2),
#             transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#             transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#             # å›ºå®šé¢„å¤„ç†
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ]
#         return transforms.Compose(transform_list)

#     def _load_gan_weights(self, weight_path):
#         """åŠ è½½GANç”Ÿæˆå™¨æƒé‡"""
#         if os.path.exists(weight_path):
#             try:
#                 self.generator.load_state_dict(torch.load(weight_path, map_location=device))
#                 self.generator.eval()
#                 logger.info(f"æˆåŠŸåŠ è½½GANæƒé‡ï¼š{weight_path}")
#                 return True
#             except Exception as e:
#                 logger.error(f"GANæƒé‡åŠ è½½å¤±è´¥ï¼š{e}")
#                 return False
#         else:
#             logger.warning(f"æœªæ‰¾åˆ°GANæƒé‡æ–‡ä»¶ï¼š{weight_path}")
#             return False

#     def traditional_augment(self, image: Image.Image) -> Image.Image:
#         """ç”Ÿæˆä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼ºçš„æ ·æœ¬"""
#         img_tensor = self.traditional_transform(image)
#         return self.inv_transform(img_tensor)

#     def gan_augment(self, image: Image.Image) -> Image.Image | None:
#         """ç”ŸæˆGANå¢å¼ºçš„æ ·æœ¬ï¼ˆèåˆåŸå§‹å›¾åƒç‰¹å¾ï¼‰"""
#         if not self.gan_available:
#             return None
#         with torch.no_grad():
#             img_tensor = self.gan_preprocess(image).unsqueeze(0).to(device)
#             z = torch.randn(1, 100, device=device)
#             gen_img = self.generator(z)
#             fused_img = 0.6 * gen_img + 0.4 * img_tensor  # åŠ æƒèåˆï¼Œä¿ç•™åŸå§‹ç‰¹å¾
#             return self.inv_transform(fused_img.squeeze(0).cpu())

#     def augment(self, image: Image.Image, use_gan: bool = True) -> list[Image.Image]:
#         """ç»Ÿä¸€å¢å¼ºæ¥å£ï¼šè¿”å›ä¼ ç»Ÿå¢å¼º+GANå¢å¼ºçš„æ ·æœ¬åˆ—è¡¨"""
#         aug_imgs = [self.traditional_augment(image)]  # è‡³å°‘1ä¸ªä¼ ç»Ÿå¢å¼ºæ ·æœ¬
#         if use_gan and self.gan_available:
#             gan_img = self.gan_augment(image)
#             if gan_img:
#                 aug_imgs.append(gan_img)
#         return aug_imgs


# # ===================== 6. ä¸»æµç¨‹ï¼šGANè®­ç»ƒ+å…¨é‡æ•°æ®å¢å¼ºã€æ— ä¿®æ”¹ã€‘ =====================
# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 10,
#     img_size: tuple = (32, 32),
#     train_gan: bool = True,
#     gan_epochs: int = 100,
#     use_auto_aug: bool = True,
#     use_rand_aug: bool = True,
#     use_gan: bool = True,
#     use_mixup_in_gan: bool = True
# ):
#     """
#     å®Œæ•´æ•°æ®å¢å¼ºæµç¨‹ï¼š
#     1. è®­ç»ƒGANï¼ˆå¯é€‰ï¼‰
#     2. ç”¨ä¼ ç»Ÿ+è‡ªåŠ¨åŒ–+GANå¢å¼ºç”Ÿæˆæ ·æœ¬
#     3. ä¿ç•™åŸå§‹ç›®å½•ç»“æ„ä¿å­˜ç»“æœ
#     """
#     weight_path = "generator_weights.pth"

#     # æ­¥éª¤1ï¼šè®­ç»ƒGANï¼ˆè‹¥éœ€è¦ï¼‰
#     if train_gan or not os.path.exists(weight_path):
#         trainer = GANTrainer(
#             data_dir=input_dir,
#             img_size=img_size,
#             epochs=gan_epochs,
#             use_mixup=use_mixup_in_gan,
#             weight_path=weight_path
#         )
#         trainer.train()
#     else:
#         logger.info("æ£€æµ‹åˆ°å·²æœ‰GANæƒé‡ï¼Œè·³è¿‡è®­ç»ƒ")

#     # æ­¥éª¤2ï¼šåˆå§‹åŒ–å…¨é‡å¢å¼ºå™¨
#     augmenter = FullAugmenter(
#         img_size=img_size,
#         use_auto_aug=use_auto_aug,
#         use_rand_aug=use_rand_aug,
#         gan_weight_path=weight_path,
#         use_gan=use_gan
#     )

#     # æ­¥éª¤3ï¼šæ‰¹é‡å¤„ç†å›¾åƒ
#     input_path = Path(input_dir)
#     output_path = Path(output_dir)
#     output_path.mkdir(parents=True, exist_ok=True)

#     # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶
#     image_extensions = ('.png', '.jpg', '.jpeg', '.bmp')
#     image_files = [f for f in input_path.rglob('*') if f.suffix.lower() in image_extensions]
#     logger.info(f"å…±æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒï¼Œå¼€å§‹å…¨é‡å¢å¼º...")

#     for img_path in image_files:
#         try:
#             image = Image.open(img_path).convert('RGB')
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æ— æ•ˆå›¾åƒ {img_path}ï¼š{e}")
#             continue

#         # ä¿æŒåŸå§‹ç›®å½•ç»“æ„
#         rel_dir = img_path.parent.relative_to(input_path)
#         target_dir = output_path / rel_dir
#         target_dir.mkdir(parents=True, exist_ok=True)

#         # ä¿å­˜åŸå§‹å›¾åƒ
#         orig_path = target_dir / f"orig_{img_path.name}"
#         image.save(orig_path)

#         # ç”Ÿæˆå¢å¼ºå›¾åƒï¼ˆæŒ‰æŒ‡å®šæ•°é‡ç”Ÿæˆï¼‰
#         for i in range(augmentations_per_image):
#             # éšæœºé€‰æ‹©å¢å¼ºç»„åˆï¼ˆ70%ä¼ ç»Ÿ+30%GANï¼Œæ— GANåˆ™å…¨ç”¨ä¼ ç»Ÿï¼‰
#             use_gan_flag = use_gan and augmenter.gan_available and random.random() < 1
#             aug_imgs = augmenter.augment(image, use_gan=use_gan_flag)
            
#             # ä¿å­˜å¢å¼ºæ ·æœ¬ï¼ˆç¡®ä¿æ•°é‡è¾¾æ ‡ï¼‰
#             for j, aug_img in enumerate(aug_imgs):
#                 if i * 2 + j >= augmentations_per_image:
#                     break  # é¿å…è¶…å‡ºæŒ‡å®šæ•°é‡
#                 aug_save_path = target_dir / f"aug_{i}_{j}_{img_path.name}"
#                 aug_img.save(aug_save_path)

#     logger.info(f"å…¨é‡æ•°æ®å¢å¼ºå®Œæˆï¼ç»“æœä¿å­˜è‡³ {output_dir}")

#74.58
# import os 
# import random
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# from torchvision import transforms
# import albumentations as A
# from albumentations.pytorch import ToTensorV2
# import cv2
# from PIL import Image
# from pathlib import Path
# import logging
# from tqdm import tqdm

# # é…ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(name)s-%(levelname)s-%(message)s')
# logger = logging.getLogger(__name__)

# # è®¾å¤‡è®¾ç½®ï¼ˆä¼˜å…ˆGPUâ†’MPSâ†’CPUï¼‰
# def get_device():
#     if torch.cuda.is_available():
#         return torch.device("cuda")
#     elif torch.backends.mps.is_available():
#         return torch.device("mps")
#     else:
#         return torch.device("cpu")

# device = get_device()
# logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")


# # ===================== 1. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆMixup/CutMixï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# def mixup_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size = x.size(0)
#     index = torch.randperm(batch_size).to(x.device)
#     mixed_x = lam * x + (1 - lam) * x[index, :]
#     if y is None:
#         return mixed_x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return mixed_x, y_a, y_b, lam

# def cutmix_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size, _, H, W = x.size()
#     cut_rat = np.sqrt(1. - lam)
#     cut_w = int(W * cut_rat)
#     cut_h = int(H * cut_rat)
#     cx = np.random.randint(W)
#     cy = np.random.randint(H)
#     bbx1 = np.clip(cx - cut_w // 2, 0, W)
#     bby1 = np.clip(cy - cut_h // 2, 0, H)
#     bbx2 = np.clip(cx + cut_w // 2, 0, W)
#     bby2 = np.clip(cy + cut_h // 2, 0, H)
#     index = torch.randperm(batch_size).to(x.device)
#     x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
#     lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (H * W))
#     if y is None:
#         return x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return x, y_a, y_b, lam


# # ===================== 2. æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒä¼ ç»Ÿ/è‡ªåŠ¨åŒ–å¢å¼ºï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# class ImageDataset(Dataset):
#     def __init__(
#         self, 
#         root_dir, 
#         img_size=(32, 32), 
#         use_auto_aug=False,
#         use_rand_aug=False,
#         is_train=True
#     ):
#         self.root_dir = root_dir
#         self.img_size = img_size
#         self.is_train = is_train
#         self.image_paths = [
#             p for p in Path(root_dir).glob('**/*') 
#             if p.suffix.lower() in ('.png', '.jpg', '.jpeg', '.bmp')
#         ]
#         logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.image_paths)} å¼ å›¾åƒï¼ˆè®­ç»ƒæ¨¡å¼ï¼š{is_train}ï¼‰")
#         self.transform = self._build_transform(use_auto_aug, use_rand_aug)

#     def _build_transform(self, use_auto_aug, use_rand_aug):
#         transform_list = []
#         if self.is_train:
#             transform_list.extend([
#                 transforms.RandomResizedCrop(size=self.img_size, scale=(0.08, 1.0), ratio=(3/4, 4/3)),
#                 transforms.RandomHorizontalFlip(p=0.5),
#                 transforms.RandomVerticalFlip(p=0.2),
#                 transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#                 transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#                 transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#                 transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 3.0))], p=0.2),
#                 transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#                 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)
#             ])
#         transform_list.extend([
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ])
#         return transforms.Compose(transform_list)

#     def __len__(self):
#         return len(self.image_paths)

#     def __getitem__(self, idx):
#         img_path = self.image_paths[idx]
#         try:
#             image = Image.open(img_path).convert('RGB')
#             return self.transform(image)
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æŸåå›¾åƒ {img_path}ï¼š{e}")
#             return torch.randn(3, self.img_size[0], self.img_size[1])


# # ===================== 3. GANæ¨¡å‹å®šä¹‰ï¼ˆå½»åº•ç§»é™¤å™ªå£°ç›¸å…³ï¼‰ã€æ ¸å¿ƒä¿®æ”¹ã€‘ =====================
# class SelfAttention(nn.Module):
#     """ä¿ç•™è‡ªæ³¨æ„åŠ›ï¼Œæ— ä¿®æ”¹"""
#     def __init__(self, in_dim):
#         super().__init__()
#         self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.key_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.value_conv = nn.Conv2d(in_dim, in_dim, 1)
#         self.gamma = nn.Parameter(torch.zeros(1))
#         self.softmax = nn.Softmax(dim=-1)

#     def forward(self, x):
#         batch_size, C, w, h = x.size()
#         proj_query = self.query_conv(x).view(batch_size, -1, w*h).permute(0, 2, 1)
#         proj_key = self.key_conv(x).view(batch_size, -1, w*h)
#         energy = torch.bmm(proj_query, proj_key)
#         attention = self.softmax(energy)
#         proj_value = self.value_conv(x).view(batch_size, -1, w*h)
#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))
#         out = out.view(batch_size, C, w, h)
#         return self.gamma * out + x


# class Generator(nn.Module):
#     """ã€ä¿®æ”¹1ã€‘ç§»é™¤latent_dimå’Œå™ªå£°è¾“å…¥ï¼Œæ”¹ä¸ºå›ºå®šç»´åº¦çš„è¾“å…¥å‘é‡"""
#     def __init__(self, channels=3, img_size=32, input_dim=100):
#         # ç”¨input_dimï¼ˆå›ºå®šè¾“å…¥ç»´åº¦ï¼‰æ›¿ä»£latent_dimï¼Œä¸å†ä¾èµ–å¤–éƒ¨å™ªå£°
#         super().__init__()
#         self.init_size = img_size // 4
#         self.input_dim = input_dim  # å›ºå®šè¾“å…¥ç»´åº¦ï¼ˆæ›¿ä»£å™ªå£°ç»´åº¦ï¼‰
#         # ã€ä¿®æ”¹2ã€‘çº¿æ€§å±‚è¾“å…¥ç»´åº¦æ”¹ä¸ºinput_dimï¼ˆæ— å™ªå£°ï¼Œä»…ç”¨å›ºå®šç»´åº¦å‘é‡ï¼‰
#         self.l1 = nn.Sequential(nn.Linear(self.input_dim, 128 * self.init_size ** 2))

#         self.conv_blocks = nn.Sequential(
#             nn.BatchNorm2d(128),
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 128, 3, 1, 1),
#             nn.BatchNorm2d(128, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 64, 3, 1, 1),
#             nn.BatchNorm2d(64, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             SelfAttention(64),
            
#             nn.Conv2d(64, channels, 3, 1, 1),
#             nn.Tanh()
#         )

#     def forward(self):
#         """ã€ä¿®æ”¹3ã€‘æ— è¾“å…¥å‚æ•°ï¼Œå†…éƒ¨ç”Ÿæˆå›ºå®šç»´åº¦çš„éšæœºå‘é‡ï¼ˆæ›¿ä»£å¤–éƒ¨å™ªå£°zï¼‰"""
#         # å†…éƒ¨ç”Ÿæˆéšæœºå‘é‡ï¼ˆä»…ç”¨äºæ¨¡å‹å‰å‘ï¼Œæ— å¤–éƒ¨å™ªå£°ä¾èµ–ï¼‰
#         x = torch.randn(1, self.input_dim, device=device)  # å•æ ·æœ¬ç”Ÿæˆ
#         out = self.l1(x)
#         out = out.view(out.shape[0], 128, self.init_size, self.init_size)
#         return self.conv_blocks(out)


# class Discriminator(nn.Module):
#     """ã€æ— ä¿®æ”¹ã€‘åˆ¤åˆ«å™¨ä¸æ¶‰åŠå™ªå£°ï¼Œä¿æŒåŸç»“æ„"""
#     def __init__(self, channels=3, img_size=32):
#         super().__init__()
#         def discriminator_block(in_filters, out_filters, bn=True):
#             block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True)]
#             if bn:
#                 block.append(nn.BatchNorm2d(out_filters, 0.8))
#             return block

#         self.model = nn.Sequential(
#             *discriminator_block(channels, 16, bn=False),
#             *discriminator_block(16, 32),
#             *discriminator_block(32, 64),
#             *discriminator_block(64, 128),
#         )

#         ds_size = img_size // (2**4)
#         self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size**2, 1), nn.Sigmoid())

#     def forward(self, img):
#         out = self.model(img)
#         out = out.view(out.shape[0], -1)
#         return self.adv_layer(out)


# # ===================== 4. GANè®­ç»ƒå™¨ï¼ˆå½»åº•ç§»é™¤å™ªå£°ç›¸å…³ï¼‰ã€æ ¸å¿ƒä¿®æ”¹ã€‘ =====================
# class GANTrainer:
#     def __init__(
#         self,
#         data_dir,
#         img_size=(32, 32),
#         epochs=30,
#         batch_size=64,
#         lr=0.0002,
#         weight_path="generator_weights.pth",
#         use_mixup=True,
#         use_cutmix=True,
#         generator_input_dim=100  # ã€æ–°å¢ã€‘ç”Ÿæˆå™¨å›ºå®šè¾“å…¥ç»´åº¦ï¼ˆæ›¿ä»£latent_dimï¼‰
#     ):
#         self.data_dir = data_dir
#         self.img_size = img_size
#         self.epochs = epochs
#         self.batch_size = batch_size
#         self.lr = lr
#         self.weight_path = weight_path
#         self.use_mixup = use_mixup
#         self.use_cutmix = use_cutmix
#         self.generator_input_dim = generator_input_dim

#         # ã€ä¿®æ”¹1ã€‘åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼šæ— latent_dimï¼Œä¼ generator_input_dim
#         self.generator = Generator(
#             img_size=img_size[0],
#             input_dim=self.generator_input_dim
#         ).to(device)
#         self.discriminator = Discriminator(img_size=img_size[0]).to(device)

#         # æŸå¤±ä¸ä¼˜åŒ–å™¨ï¼ˆæ— ä¿®æ”¹ï¼‰
#         self.adversarial_loss = nn.BCELoss().to(device)
#         self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))
#         self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))

#         # åŠ è½½æ•°æ®é›†ï¼ˆæ— ä¿®æ”¹ï¼‰
#         self.dataset = ImageDataset(
#             root_dir=data_dir,
#             img_size=img_size,
#             use_rand_aug=True,
#             is_train=True
#         )
#         self.dataloader = DataLoader(
#             self.dataset,
#             batch_size=batch_size,
#             shuffle=True,
#             num_workers=2,
#             pin_memory=True
#         )

#     def train(self):
#         logger.info(f"å¼€å§‹GANè®­ç»ƒï¼ˆMixup: {self.use_mixup}, CutMix: {self.use_cutmix}ï¼‰ï¼Œå…± {self.epochs} è½®")
        
#         for epoch in range(self.epochs):
#             pbar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.epochs}")
#             for imgs in pbar:
#                 batch_size = imgs.size(0)
#                 valid = torch.ones(batch_size, 1).to(device)
#                 fake = torch.zeros(batch_size, 1).to(device)
#                 real_imgs = imgs.to(device)

#                 # Mixup/CutMixå¤„ç†ï¼ˆæ— ä¿®æ”¹ï¼‰
#                 if self.use_mixup:
#                     real_imgs, _, _, _ = mixup_data(real_imgs, None, alpha=0.1)
#                 if self.use_cutmix and not self.use_mixup:
#                     real_imgs, _, _, _ = cutmix_data(real_imgs, None, alpha=0.1)

#                 # ----------------- è®­ç»ƒç”Ÿæˆå™¨ã€ä¿®æ”¹2ã€‘ç§»é™¤å™ªå£°zï¼Œç›´æ¥è°ƒç”¨generator() -----------------
#                 self.optimizer_G.zero_grad()
#                 # æ— å¤–éƒ¨å™ªå£°ï¼Œç”Ÿæˆå™¨å†…éƒ¨ç”Ÿæˆè¾“å…¥å‘é‡
#                 gen_imgs = torch.cat([self.generator() for _ in range(batch_size)], dim=0)  # æ‰¹é‡ç”Ÿæˆ
#                 g_loss = self.adversarial_loss(self.discriminator(gen_imgs), valid)
#                 g_loss.backward()
#                 self.optimizer_G.step()

#                 # ----------------- è®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆæ— ä¿®æ”¹ï¼‰ -----------------
#                 self.optimizer_D.zero_grad()
#                 real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)
#                 fake_loss = self.adversarial_loss(self.discriminator(gen_imgs.detach()), fake)
#                 d_loss = (real_loss + fake_loss) / 2
#                 d_loss.backward()
#                 self.optimizer_D.step()

#                 pbar.set_postfix({"DæŸå¤±": d_loss.item(), "GæŸå¤±": g_loss.item()})

#             # ä¿å­˜æƒé‡ï¼ˆæ— ä¿®æ”¹ï¼‰
#             if (epoch + 1) % 10 == 0:
#                 torch.save({
#                     "generator_state_dict": self.generator.state_dict(),
#                     "input_dim": self.generator_input_dim  # ä¿å­˜è¾“å…¥ç»´åº¦ï¼Œåç»­åŠ è½½ç”¨
#                 }, f"generator_weights_epoch_{epoch+1}.pth")
#                 logger.info(f"å·²ä¿å­˜ç¬¬ {epoch+1} è½®GANæƒé‡")

#         torch.save({
#             "generator_state_dict": self.generator.state_dict(),
#             "input_dim": self.generator_input_dim
#         }, self.weight_path)
#         logger.info(f"GANè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæƒé‡ä¿å­˜è‡³ {self.weight_path}")


# # ===================== 5. å®Œæ•´å¢å¼ºå™¨ï¼ˆå½»åº•ç§»é™¤å™ªå£°ç›¸å…³ï¼‰ã€æ ¸å¿ƒä¿®æ”¹ã€‘ =====================
# class FullAugmenter:
#     def __init__(
#         self,
#         img_size=(32, 32),
#         use_auto_aug=False,
#         use_rand_aug=True,
#         gan_weight_path="generator_weights.pth",
#         use_gan=True
#     ):
#         self.img_size = img_size
#         self.use_gan = use_gan

#         # ä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼ºï¼ˆæ— ä¿®æ”¹ï¼‰
#         self.traditional_transform = self._build_traditional_transform(use_auto_aug, use_rand_aug)
#         self.inv_transform = transforms.Compose([
#             transforms.Normalize(mean=(-1.0, -1.0, -1.0), std=(2.0, 2.0, 2.0)),
#             transforms.ToPILImage()
#         ])

#         # ã€ä¿®æ”¹1ã€‘GANåˆå§‹åŒ–ï¼šç§»é™¤latent_dimï¼ŒåŠ è½½input_dim
#         self.gan_available = False
#         self.generator = None
#         self.generator_input_dim = 100  # é»˜è®¤è¾“å…¥ç»´åº¦
#         if use_gan:
#             self.gan_available = self._load_gan_weights(gan_weight_path)
#             if self.gan_available:
#                 self.gan_preprocess = transforms.Compose([
#                     transforms.Resize(img_size),
#                     transforms.ToTensor(),
#                     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#                 ])

#     def _build_traditional_transform(self, use_auto_aug, use_rand_aug):
#         """æ— ä¿®æ”¹ï¼Œä¿ç•™ä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼º"""
#         transform_list = [
#             transforms.RandomResizedCrop(size=self.img_size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),
#             transforms.RandomHorizontalFlip(p=0.5),
#             transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#             transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#             transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#             transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.2),
#             transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#             transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ]
#         return transforms.Compose(transform_list)

#     def _load_gan_weights(self, weight_path):
#         """ã€ä¿®æ”¹2ã€‘åŠ è½½æƒé‡æ—¶è¯»å–input_dimï¼Œæ— latent_dim"""
#         if not os.path.exists(weight_path):
#             logger.warning(f"æœªæ‰¾åˆ°GANæƒé‡æ–‡ä»¶ï¼š{weight_path}")
#             return False
#         try:
#             checkpoint = torch.load(weight_path, map_location=device)
#             self.generator_input_dim = checkpoint.get("input_dim", 100)
#             # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼ˆæ— latent_dimï¼Œä¼ input_dimï¼‰
#             self.generator = Generator(
#                 img_size=self.img_size[0],
#                 input_dim=self.generator_input_dim
#             ).to(device)
#             self.generator.load_state_dict(checkpoint["generator_state_dict"])
#             self.generator.eval()
#             logger.info(f"æˆåŠŸåŠ è½½GANæƒé‡ï¼š{weight_path}ï¼Œè¾“å…¥ç»´åº¦ï¼š{self.generator_input_dim}")
#             return True
#         except Exception as e:
#             logger.error(f"GANæƒé‡åŠ è½½å¤±è´¥ï¼š{e}")
#             return False

#     def traditional_augment(self, image: Image.Image) -> Image.Image:
#         """æ— ä¿®æ”¹ï¼Œä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼º"""
#         img_tensor = self.traditional_transform(image)
#         return self.inv_transform(img_tensor)

#     def gan_augment(self, image: Image.Image) -> Image.Image | None:
#         """ã€ä¿®æ”¹3ã€‘ç§»é™¤å™ªå£°zï¼Œç›´æ¥è°ƒç”¨generator()ç”Ÿæˆæ ·æœ¬"""
#         if not self.gan_available or self.generator is None:
#             return None
#         with torch.no_grad():
#             # æ— å¤–éƒ¨å™ªå£°ï¼Œç”Ÿæˆå™¨å†…éƒ¨ç”Ÿæˆæ ·æœ¬
#             gen_img = self.generator()  # å•æ ·æœ¬ç”Ÿæˆ
#             # èåˆåŸå§‹å›¾åƒç‰¹å¾ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
#             img_tensor = self.gan_preprocess(image).unsqueeze(0).to(device)
#             fused_img = 0.6 * gen_img + 0.4 * img_tensor
#             return self.inv_transform(fused_img.squeeze(0).cpu())

#     def augment(self, image: Image.Image, use_gan: bool = True) -> list[Image.Image]:
#         """æ— ä¿®æ”¹ï¼Œç»Ÿä¸€å¢å¼ºæ¥å£"""
#         aug_imgs = [self.traditional_augment(image)]
#         if use_gan and self.gan_available:
#             gan_img = self.gan_augment(image)
#             if gan_img:
#                 aug_imgs.append(gan_img)
#         return aug_imgs


# # ===================== 6. ä¸»æµç¨‹ï¼šGANè®­ç»ƒ+å…¨é‡æ•°æ®å¢å¼ºã€ä¿®æ”¹ï¼šç§»é™¤latent_dimå‚æ•°ã€‘ =====================
# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 10,
#     img_size: tuple = (32, 32),
#     train_gan: bool = True,
#     gan_epochs: int = 30,
#     use_auto_aug: bool = True,
#     use_rand_aug: bool = True,
#     use_gan: bool = True,
#     use_mixup_in_gan: bool = True,
#     generator_input_dim=100  # ã€æ–°å¢ã€‘ç”Ÿæˆå™¨è¾“å…¥ç»´åº¦ï¼ˆæ›¿ä»£latent_dimï¼‰
# ):
#     weight_path = "generator_weights.pth"

#     # æ­¥éª¤1ï¼šè®­ç»ƒGANï¼ˆæ— latent_dimï¼Œä¼ generator_input_dimï¼‰
#     if train_gan or not os.path.exists(weight_path):
#         trainer = GANTrainer(
#             data_dir=input_dir,
#             img_size=img_size,
#             epochs=gan_epochs,
#             use_mixup=use_mixup_in_gan,
#             weight_path=weight_path,
#             generator_input_dim=generator_input_dim
#         )
#         trainer.train()
#     else:
#         logger.info("æ£€æµ‹åˆ°å·²æœ‰GANæƒé‡ï¼Œè·³è¿‡è®­ç»ƒ")

#     # æ­¥éª¤2ï¼šåˆå§‹åŒ–å¢å¼ºå™¨ï¼ˆæ— ä¿®æ”¹ï¼‰
#     augmenter = FullAugmenter(
#         img_size=img_size,
#         use_auto_aug=use_auto_aug,
#         use_rand_aug=use_rand_aug,
#         gan_weight_path=weight_path,
#         use_gan=use_gan
#     )

#     # æ­¥éª¤3ï¼šæ‰¹é‡å¤„ç†å›¾åƒï¼ˆæ— ä¿®æ”¹ï¼‰
#     input_path = Path(input_dir)
#     output_path = Path(output_dir)
#     output_path.mkdir(parents=True, exist_ok=True)

#     image_extensions = ('.png', '.jpg', '.jpeg', '.bmp')
#     image_files = [f for f in input_path.rglob('*') if f.suffix.lower() in image_extensions]
#     logger.info(f"å…±æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒï¼Œå¼€å§‹å…¨é‡å¢å¼º...")

#     for img_path in image_files:
#         try:
#             image = Image.open(img_path).convert('RGB')
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æ— æ•ˆå›¾åƒ {img_path}ï¼š{e}")
#             continue

#         rel_dir = img_path.parent.relative_to(input_path)
#         target_dir = output_path / rel_dir
#         target_dir.mkdir(parents=True, exist_ok=True)

#         orig_path = target_dir / f"orig_{img_path.name}"
#         image.save(orig_path)

#         for i in range(augmentations_per_image):
#             use_gan_flag = use_gan and augmenter.gan_available and random.random() < 1
#             aug_imgs = augmenter.augment(image, use_gan=use_gan_flag)
            
#             for j, aug_img in enumerate(aug_imgs):
#                 if i * 2 + j >= augmentations_per_image:
#                     break
#                 aug_save_path = target_dir / f"aug_{i}_{j}_{img_path.name}"
#                 aug_img.save(aug_save_path)

#     logger.info(f"å…¨é‡æ•°æ®å¢å¼ºå®Œæˆï¼ç»“æœä¿å­˜è‡³ {output_dir}")

#styleGAN 0.69
# import os
# import random
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# from torchvision import transforms
# import cv2
# from PIL import Image
# from pathlib import Path
# import logging
# from tqdm import tqdm

# # é…ç½®æ—¥å¿—ï¼ˆä»…è¾“å‡ºå…³é”®ä¿¡æ¯ï¼Œé¿å…å†—ä½™ï¼‰
# logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(name)s-INFO-%(message)s')
# logger = logging.getLogger(__name__)

# # è®¾å¤‡è®¾ç½®ï¼ˆä¼˜å…ˆGPUâ†’MPSâ†’CPUï¼‰
# def get_device():
#     if torch.cuda.is_available():
#         return torch.device("cuda")
#     elif torch.backends.mps.is_available():
#         return torch.device("mps")
#     else:
#         return torch.device("cpu")

# device = get_device()
# logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")


# # ===================== 1. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆMixup/CutMixï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# def mixup_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size = x.size(0)
#     index = torch.randperm(batch_size).to(x.device)
#     mixed_x = lam * x + (1 - lam) * x[index, :]
#     if y is None:
#         return mixed_x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return mixed_x, y_a, y_b, lam

# def cutmix_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size, _, H, W = x.size()
#     cut_rat = np.sqrt(1. - lam)
#     cut_w = int(W * cut_rat)
#     cut_h = int(H * cut_rat)
#     cx = np.random.randint(W)
#     cy = np.random.randint(H)
#     bbx1 = np.clip(cx - cut_w // 2, 0, W)
#     bby1 = np.clip(cy - cut_h // 2, 0, H)
#     bbx2 = np.clip(cx + cut_w // 2, 0, W)
#     bby2 = np.clip(cy + cut_h // 2, 0, H)
#     index = torch.randperm(batch_size).to(x.device)
#     x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
#     lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (H * W))
#     if y is None:
#         return x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return x, y_a, y_b, lam


# # ===================== 2. æ•°æ®åŠ è½½å™¨ï¼ˆ32Ã—32ï¼‰ã€æ— ä¿®æ”¹ï¼Œä»…ä¿ç•™å¿…è¦æ—¥å¿—ã€‘ =====================
# class ImageDataset(Dataset):
#     def __init__(
#         self, 
#         root_dir, 
#         img_size=(32, 32),
#         use_auto_aug=False,
#         use_rand_aug=False,
#         is_train=True
#     ):
#         self.root_dir = root_dir
#         self.img_size = img_size
#         self.is_train = is_train
#         self.image_paths = [
#             p for p in Path(root_dir).glob('**/*') 
#             if p.suffix.lower() in ('.png', '.jpg', '.jpeg', '.bmp')
#         ]
#         logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼šå…± {len(self.image_paths)} å¼ å›¾åƒï¼Œå°ºå¯¸ {img_size}ï¼Œè®­ç»ƒæ¨¡å¼ {is_train}")
#         self.transform = self._build_transform(use_auto_aug, use_rand_aug)

#     def _build_transform(self, use_auto_aug, use_rand_aug):
#         transform_list = []
#         if self.is_train:
#             transform_list.extend([
#                 transforms.RandomResizedCrop(size=self.img_size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),
#                 transforms.RandomHorizontalFlip(p=0.5),
#                 transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#                 transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#                 transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#                 transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 3.0))], p=0.2),
#                 transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#                 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)
#             ])
#         transform_list.extend([
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ])
#         return transforms.Compose(transform_list)

#     def __len__(self):
#         return len(self.image_paths)

#     def __getitem__(self, idx):
#         img_path = self.image_paths[idx]
#         try:
#             image = Image.open(img_path).convert('RGB')
#             return self.transform(image)
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æŸåå›¾åƒï¼š{img_path}ï¼ŒåŸå› ï¼š{str(e)}")
#             return torch.randn(3, self.img_size[0], self.img_size[1])


# # ===================== 3. StyleGANæ ¸å¿ƒæ¨¡å—ï¼ˆåˆ é™¤æ‰€æœ‰è°ƒè¯•æ‰“å°ï¼‰ =====================
# class SelfAttention(nn.Module):
#     def __init__(self, in_dim):
#         super().__init__()
#         self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.key_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.value_conv = nn.Conv2d(in_dim, in_dim, 1)
#         self.gamma = nn.Parameter(torch.zeros(1))
#         self.softmax = nn.Softmax(dim=-1)

#     def forward(self, x):
#         batch_size, C, w, h = x.size()
#         proj_query = self.query_conv(x).view(batch_size, -1, w*h).permute(0, 2, 1)
#         proj_key = self.key_conv(x).view(batch_size, -1, w*h)
#         energy = torch.bmm(proj_query, proj_key)
#         attention = self.softmax(energy)
#         proj_value = self.value_conv(x).view(batch_size, -1, w*h)
#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))
#         out = out.view(batch_size, C, w, h)
#         return self.gamma * out + x


# class AdaIN(nn.Module):
#     def __init__(self, in_channels, style_dim):
#         super().__init__()
#         self.instance_norm = nn.InstanceNorm2d(in_channels, affine=False)
#         self.style_scale = nn.Linear(style_dim, in_channels)
#         self.style_shift = nn.Linear(style_dim, in_channels)
#         self.style_scale.weight.data.uniform_()
#         self.style_scale.bias.data.fill_(1.0)
#         self.style_shift.bias.data.fill_(0.0)

#     def forward(self, x, style_w):
#         x_norm = self.instance_norm(x)
#         scale = self.style_scale(style_w).view(-1, x_norm.size(1), 1, 1)
#         shift = self.style_shift(style_w).view(-1, x_norm.size(1), 1, 1)
#         return x_norm * scale + shift


# class StyleMappingNetwork(nn.Module):
#     def __init__(self, z_dim=512, w_dim=512, num_layers=8):
#         super().__init__()
#         self.z_dim = z_dim
#         self.w_dim = w_dim
#         layers = []
#         for _ in range(num_layers):
#             layers.append(nn.Linear(w_dim, w_dim))
#             layers.append(nn.LeakyReLU(0.2, inplace=True))
#         self.mapping = nn.Sequential(*layers)

#     def forward(self, batch_size):
#         z = torch.randn(batch_size, self.z_dim, device=device)
#         w = self.mapping(z)
#         return w


# class StyleGANGenerator(nn.Module):
#     """32Ã—32ç”Ÿæˆå™¨ï¼ˆæ— ä»»ä½•è°ƒè¯•æ‰“å°ï¼‰"""
#     def __init__(self, w_dim=512, img_size=32, channels=3):
#         super().__init__()
#         self.w_dim = w_dim
#         self.img_size = img_size
#         self.num_up_layers = 3  # 4â†’8â†’16â†’32ï¼ˆ3æ¬¡ä¸Šé‡‡æ ·ï¼‰

#         # åˆå§‹å·ç§¯ï¼š1Ã—1â†’4Ã—4ï¼ˆ256é€šé“ï¼‰
#         self.init_conv = nn.Conv2d(1, 256, kernel_size=4, padding=3)
#         self.init_norm = nn.InstanceNorm2d(256, affine=True)
#         self.init_act = nn.LeakyReLU(0.2, inplace=True)

#         # ä¸Šé‡‡æ ·å±‚ï¼ˆå›ºå®šé€šé“å’Œå°ºå¯¸å˜åŒ–ï¼‰
#         self.synthesis_layers = nn.ModuleList([
#             nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
#             nn.Conv2d(256, 128, 3, padding=1),
#             AdaIN(128, w_dim),
#             nn.LeakyReLU(0.2, inplace=True),
#             nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
#             nn.Conv2d(128, 64, 3, padding=1),
#             AdaIN(64, w_dim),
#             nn.LeakyReLU(0.2, inplace=True),
#             nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
#             nn.Conv2d(64, 64, 3, padding=1),
#             AdaIN(64, w_dim),
#             nn.LeakyReLU(0.2, inplace=True),
#         ])

#         # è‡ªæ³¨æ„åŠ›+è¾“å‡ºå·ç§¯
#         self.attention = SelfAttention(64)
#         self.final_conv = nn.Conv2d(64, channels, 1, padding=0)
#         self.tanh = nn.Tanh()

#         # æ˜ å°„ç½‘ç»œ
#         self.mapping_net = StyleMappingNetwork(z_dim=w_dim, w_dim=w_dim)

#     def forward(self, batch_size=1):
#         w = self.mapping_net(batch_size)
#         # åˆå§‹ç‰¹å¾ï¼š1Ã—1â†’4Ã—4
#         x = torch.ones(batch_size, 1, 1, 1, device=device)
#         x = self.init_conv(x)
#         x = self.init_norm(x)
#         x = self.init_act(x)
#         # ä¸Šé‡‡æ ·åˆ°32Ã—32
#         for layer in self.synthesis_layers:
#             if isinstance(layer, AdaIN):
#                 x = layer(x, w)
#             else:
#                 x = layer(x)
#         # è‡ªæ³¨æ„åŠ›+è¾“å‡º
#         x = self.attention(x)
#         gen_imgs = self.final_conv(x)
#         gen_imgs = self.tanh(gen_imgs)
#         return gen_imgs


# class StyleGANDiscriminator(nn.Module):
#     """32Ã—32åˆ¤åˆ«å™¨ï¼ˆæ— ä»»ä½•è°ƒè¯•æ‰“å°ï¼‰"""
#     def __init__(self, img_size=32, channels=3):
#         super().__init__()
#         self.img_size = img_size
#         # å›ºå®š4ä¸ªå°ºåº¦ï¼š32â†’16â†’8â†’4
#         self.discriminator_blocks = nn.ModuleList([
#             # å°ºåº¦1ï¼š32â†’16ï¼ˆ3â†’64â†’128ï¼‰
#             nn.Sequential(
#                 nn.Conv2d(channels, 64, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True),
#                 nn.Conv2d(64, 128, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True),
#                 nn.AvgPool2d(2, stride=2)
#             ),
#             # å°ºåº¦2ï¼š16â†’8ï¼ˆ128â†’256ï¼‰
#             nn.Sequential(
#                 nn.Conv2d(128, 256, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True),
#                 nn.Conv2d(256, 256, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True),
#                 nn.AvgPool2d(2, stride=2)
#             ),
#             # å°ºåº¦3ï¼š8â†’4ï¼ˆ256â†’512ï¼‰
#             nn.Sequential(
#                 nn.Conv2d(256, 512, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True),
#                 nn.Conv2d(512, 512, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True),
#                 nn.AvgPool2d(2, stride=2)
#             ),
#             # å°ºåº¦4ï¼š4Ã—4ï¼ˆ512â†’512ï¼‰
#             nn.Sequential(
#                 nn.Conv2d(512, 512, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True),
#                 nn.Conv2d(512, 512, 3, padding=1),
#                 nn.LeakyReLU(0.2, inplace=True)
#             )
#         ])
#         # çº¿æ€§å±‚ï¼š512Ã—4Ã—4=8192
#         self.final_linear = nn.Linear(512 * 4 * 4, 1)
#         self.sigmoid = nn.Sigmoid()

#     def forward(self, imgs):
#         x = imgs
#         # é€å°ºåº¦å¤„ç†
#         for block in self.discriminator_blocks:
#             x = block(x)
#         # å±•å¹³+çº¿æ€§å±‚
#         x_flat = x.view(imgs.size(0), -1)
#         output = self.final_linear(x_flat)
#         output = self.sigmoid(output)
#         return output


# # ===================== 4. StyleGANè®­ç»ƒå™¨ï¼ˆä»…ä¿ç•™è¿›åº¦æ¡å’Œå…³é”®æ—¥å¿—ï¼‰ =====================
# class StyleGANTrainer:
#     def __init__(
#         self,
#         data_dir,
#         img_size=(32, 32),
#         epochs=50,
#         batch_size=64,
#         lr=2e-4,
#         weight_path="stylegan_generator_weights_32x32.pth",
#         use_mixup=True,
#         use_cutmix=True,
#         w_dim=512
#     ):
#         self.data_dir = data_dir
#         self.img_size = img_size
#         self.epochs = epochs
#         self.batch_size = batch_size
#         self.lr = lr
#         self.weight_path = weight_path
#         self.use_mixup = use_mixup
#         self.use_cutmix = use_cutmix
#         self.w_dim = w_dim

#         # åˆå§‹åŒ–æ¨¡å‹
#         self.generator = StyleGANGenerator(w_dim=w_dim, img_size=img_size[0]).to(device)
#         self.discriminator = StyleGANDiscriminator(img_size=img_size[0]).to(device)

#         # ä¼˜åŒ–å™¨
#         self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.0, 0.99), weight_decay=1e-8)
#         self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.0, 0.99), weight_decay=1e-8)

#         # æŸå¤±å‡½æ•°
#         self.adversarial_loss = nn.BCELoss().to(device)

#         # åŠ è½½æ•°æ®é›†
#         self.dataset = ImageDataset(root_dir=data_dir, img_size=img_size, use_rand_aug=True, is_train=True)
#         self.dataloader = DataLoader(
#             self.dataset,
#             batch_size=batch_size,
#             shuffle=True,
#             num_workers=2,
#             pin_memory=True,
#             multiprocessing_context='spawn'
#         )

#     def train(self):
#         logger.info(f"StyleGANè®­ç»ƒå¼€å§‹ï¼šå°ºå¯¸ {self.img_size}ï¼Œè½®æ¬¡ {self.epochs}ï¼Œæ‰¹é‡ {self.batch_size}ï¼ŒMixup={self.use_mixup}ï¼ŒCutMix={self.use_cutmix}")
        
#         for epoch in range(self.epochs):
#             # è¿›åº¦æ¡ï¼ˆä»…æ˜¾ç¤ºè½®æ¬¡ã€è¿›åº¦ã€æŸå¤±ï¼‰
#             pbar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.epochs}", unit="batch")
#             for imgs in pbar:
#                 batch_size = imgs.size(0)
#                 valid = torch.ones(batch_size, 1).to(device)
#                 fake = torch.zeros(batch_size, 1).to(device)
#                 real_imgs = imgs.to(device)

#                 # Mixup/CutMixå¤„ç†
#                 if self.use_mixup:
#                     real_imgs, _, _, _ = mixup_data(real_imgs, None, alpha=0.1)
#                 elif self.use_cutmix:
#                     real_imgs, _, _, _ = cutmix_data(real_imgs, None, alpha=0.1)

#                 # è®­ç»ƒç”Ÿæˆå™¨
#                 self.optimizer_G.zero_grad()
#                 gen_imgs = self.generator(batch_size=batch_size)
#                 g_loss = self.adversarial_loss(self.discriminator(gen_imgs), valid)
#                 g_loss.backward()
#                 self.optimizer_G.step()

#                 # è®­ç»ƒåˆ¤åˆ«å™¨
#                 self.optimizer_D.zero_grad()
#                 d_real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)
#                 d_fake_loss = self.adversarial_loss(self.discriminator(gen_imgs.detach()), fake)
#                 d_loss = (d_real_loss + d_fake_loss) / 2
#                 d_loss.backward()
#                 self.optimizer_D.step()

#                 # è¿›åº¦æ¡æ›´æ–°æŸå¤±ï¼ˆä»…ä¿ç•™4ä½å°æ•°ï¼‰
#                 pbar.set_postfix({"D_loss": round(d_loss.item(), 4), "G_loss": round(g_loss.item(), 4)})

#             # æ¯10è½®ä¿å­˜æƒé‡
#             if (epoch + 1) % 10 == 0:
#                 save_path = f"stylegan_generator_weights_32x32_epoch_{epoch+1}.pth"
#                 torch.save({
#                     "generator_state_dict": self.generator.state_dict(),
#                     "w_dim": self.w_dim,
#                     "img_size": self.img_size[0]
#                 }, save_path)
#                 logger.info(f"ç¬¬ {epoch+1} è½®æƒé‡ä¿å­˜å®Œæˆï¼š{save_path}")

#         # è®­ç»ƒç»“æŸä¿å­˜æœ€ç»ˆæƒé‡
#         torch.save({
#             "generator_state_dict": self.generator.state_dict(),
#             "w_dim": self.w_dim,
#             "img_size": self.img_size[0]
#         }, self.weight_path)
#         logger.info(f"StyleGANè®­ç»ƒå®Œæˆï¼æœ€ç»ˆæƒé‡ä¿å­˜è‡³ï¼š{self.weight_path}")


# # ===================== 5. å®Œæ•´å¢å¼ºå™¨ï¼ˆç®€æ´ç‰ˆï¼Œæ— å†—ä½™æ‰“å°ï¼‰ =====================
# class FullAugmenter:
#     def __init__(
#         self,
#         img_size=(32, 32),
#         use_auto_aug=False,
#         use_rand_aug=True,
#         stylegan_weight_path="stylegan_generator_weights_32x32.pth",
#         use_gan=True
#     ):
#         self.img_size = img_size
#         self.use_gan = use_gan
#         self.stylegan_weight_path = stylegan_weight_path

#         # åˆå§‹åŒ–ä¼ ç»Ÿå¢å¼ºå™¨
#         self.traditional_transform = self._build_traditional_transform(use_auto_aug, use_rand_aug)
#         self.inv_transform = transforms.Compose([
#             transforms.Normalize(mean=(-1.0, -1.0, -1.0), std=(2.0, 2.0, 2.0)),
#             transforms.ToPILImage()
#         ])

#         # åˆå§‹åŒ–StyleGANå¢å¼ºå™¨ï¼ˆä»…å½“use_gan=Trueæ—¶ï¼‰
#         self.generator = None
#         if self.use_gan:
#             self._load_stylegan_model()

#     def _build_traditional_transform(self, use_auto_aug, use_rand_aug):
#         transform_list = []
#         if use_rand_aug or use_auto_aug:
#             transform_list.extend([
#                 transforms.RandomResizedCrop(size=self.img_size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),
#                 transforms.RandomHorizontalFlip(p=0.5),
#                 transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#                 transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#                 transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#                 transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 3.0))], p=0.2),
#                 transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#                 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)
#             ])
#         transform_list.extend([
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ])
#         return transforms.Compose(transform_list)

#     def _load_stylegan_model(self):
#         """åŠ è½½StyleGANæ¨¡å‹ï¼ˆä»…æ‰“å°å…³é”®æ—¥å¿—ï¼‰"""
#         if not os.path.exists(self.stylegan_weight_path):
#             logger.error(f"StyleGANæƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.stylegan_weight_path}ï¼Œå°†ç¦ç”¨GANå¢å¼º")
#             self.use_gan = False
#             return

#         try:
#             checkpoint = torch.load(self.stylegan_weight_path, map_location=device)
#             img_size = checkpoint.get("img_size", 32)
#             w_dim = checkpoint.get("w_dim", 512)

#             if img_size != self.img_size[0]:
#                 logger.error(f"StyleGANæƒé‡å°ºå¯¸ä¸åŒ¹é…ï¼šæƒé‡{img_size}Ã—{img_size}ï¼Œéœ€æ±‚{self.img_size[0]}Ã—{self.img_size[0]}ï¼Œç¦ç”¨GANå¢å¼º")
#                 self.use_gan = False
#                 return

#             # åˆå§‹åŒ–ç”Ÿæˆå™¨
#             self.generator = StyleGANGenerator(w_dim=w_dim, img_size=img_size).to(device)
#             self.generator.load_state_dict(checkpoint["generator_state_dict"])
#             self.generator.eval()  # æ¨ç†æ¨¡å¼
#             logger.info(f"StyleGANæ¨¡å‹åŠ è½½å®Œæˆï¼šæƒé‡{self.stylegan_weight_path}ï¼Œå°ºå¯¸{img_size}Ã—{img_size}")
#         except Exception as e:
#             logger.error(f"StyleGANæ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼Œç¦ç”¨GANå¢å¼º")
#             self.use_gan = False

#     def traditional_augment(self, image: Image.Image) -> Image.Image:
#         """ä¼ ç»Ÿæ•°æ®å¢å¼ºï¼ˆè¿”å›32Ã—32å›¾åƒï¼‰"""
#         img_tensor = self.traditional_transform(image)
#         return self.inv_transform(img_tensor)

#     def stylegan_augment(self, image: Image.Image) -> Image.Image | None:
#         """StyleGANå¢å¼ºï¼ˆä»…å½“æ¨¡å‹åŠ è½½æˆåŠŸæ—¶å¯ç”¨ï¼‰"""
#         if not self.use_gan or self.generator is None:
#             return None

#         with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿæ¨ç†
#             # ç”ŸæˆGANæ ·æœ¬ï¼ˆ32Ã—32ï¼‰
#             gen_img = self.generator(batch_size=1)
#             # èåˆçœŸå®å›¾åƒç‰¹å¾ï¼ˆé¿å…ç”Ÿæˆæ ·æœ¬ä¸åŸå›¾å·®å¼‚è¿‡å¤§ï¼‰
#             real_tensor = self.traditional_transform(image).unsqueeze(0).to(device)
#             fused_img = 0.6 * gen_img + 0.4 * real_tensor  # æƒé‡å¯è°ƒæ•´
#             return self.inv_transform(fused_img.squeeze(0))

#     def augment(self, image: Image.Image) -> list[Image.Image]:
#         """å•æ¬¡å¢å¼ºï¼šè¿”å›[ä¼ ç»Ÿå¢å¼ºå›¾, GANå¢å¼ºå›¾]ï¼ˆGANå¢å¼ºå¤±è´¥åˆ™ä»…è¿”å›ä¼ ç»Ÿå¢å¼ºå›¾ï¼‰"""
#         aug_imgs = [self.traditional_augment(image)]
#         if self.use_gan:
#             gan_img = self.stylegan_augment(image)
#             if gan_img is not None:
#                 aug_imgs.append(gan_img)
#         return aug_imgs


# # ===================== 6. ä¸»æµç¨‹ï¼ˆæ•°æ®å¢å¼ºå…¥å£ï¼Œç®€æ´æ—¥å¿—ï¼‰ =====================
# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 10,
#     img_size=(32, 32),
#     train_gan: bool = True,
#     gan_epochs: int = 30,
#     use_auto_aug: bool = False,
#     use_rand_aug: bool = True,
#     use_gan: bool = True,
#     use_mixup_in_gan: bool = True,
#     w_dim=512
# ):
#     # è¾“å…¥è¾“å‡ºç›®å½•æ£€æŸ¥
#     input_path = Path(input_dir)
#     output_path = Path(output_dir)
#     if not input_path.exists():
#         logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨ï¼š{input_dir}")
#         raise FileNotFoundError(f"Input directory not found: {input_dir}")
#     output_path.mkdir(parents=True, exist_ok=True)
#     logger.info(f"æ•°æ®å¢å¼ºå¼€å§‹ï¼šè¾“å…¥ç›®å½•{input_dir}ï¼Œè¾“å‡ºç›®å½•{output_dir}ï¼Œæ¯å¼ å›¾å¢å¼º{augmentations_per_image}æ¬¡")

#     # ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒStyleGANï¼ˆå¦‚éœ€ï¼‰
#     weight_path = "stylegan_generator_weights_32x32.pth"
#     if train_gan and use_gan:
#         # å¼ºåˆ¶åˆ é™¤æ—§æƒé‡ï¼ˆé¿å…ç‰ˆæœ¬å†²çªï¼‰
#         old_weights = [f for f in Path.cwd().glob("stylegan_generator_weights_32x32*.pth")]
#         if old_weights:
#             for f in old_weights:
#                 os.remove(f)
#                 logger.info(f"åˆ é™¤æ—§æƒé‡æ–‡ä»¶ï¼š{f.name}")
#         # è®­ç»ƒStyleGAN
#         trainer = StyleGANTrainer(
#             data_dir=input_dir,
#             img_size=img_size,
#             epochs=gan_epochs,
#             batch_size=64,
#             lr=2e-4,
#             weight_path=weight_path,
#             use_mixup=use_mixup_in_gan,
#             use_cutmix=use_mixup_in_gan,  # Mixup/CutMixäºŒé€‰ä¸€
#             w_dim=w_dim
#         )
#         trainer.train()
#     elif use_gan and not os.path.exists(weight_path):
#         logger.error(f"æœªå¼€å¯StyleGANè®­ç»ƒä¸”æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼š{weight_path}ï¼Œå°†ç¦ç”¨GANå¢å¼º")
#         use_gan = False

#     # ç¬¬äºŒæ­¥ï¼šåŠ è½½å¢å¼ºå™¨å¹¶æ‰§è¡Œå¢å¼º
#     augmenter = FullAugmenter(
#         img_size=img_size,
#         use_auto_aug=use_auto_aug,
#         use_rand_aug=use_rand_aug,
#         stylegan_weight_path=weight_path if use_gan else "",
#         use_gan=use_gan
#     )

#     # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶ï¼ˆä»…å¤„ç†å¸¸è§å›¾åƒæ ¼å¼ï¼‰
#     image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')
#     image_files = [f for f in input_path.rglob('*') if f.suffix.lower() in image_extensions]
#     total_images = len(image_files)
#     logger.info(f"å…±å‘ç°{total_images}å¼ å›¾åƒï¼Œå¼€å§‹æ‰¹é‡å¢å¼º...")

#     # é€å›¾å¢å¼ºå¹¶ä¿å­˜
#     for img_idx, img_file in enumerate(image_files, 1):
#         # è¯»å–å›¾åƒ
#         try:
#             image = Image.open(img_file).convert('RGB')
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æŸåå›¾åƒï¼ˆ{img_idx}/{total_images}ï¼‰ï¼š{img_file.name}ï¼ŒåŸå› ï¼š{str(e)}")
#             continue

#         # ä¿æŒåŸå§‹ç›®å½•ç»“æ„
#         relative_path = img_file.relative_to(input_path)
#         save_dir = output_path / relative_path.parent
#         save_dir.mkdir(parents=True, exist_ok=True)

#         # å•æ¬¡å›¾åƒå¢å¼ºaugmentations_per_imageæ¬¡
#         for aug_idx in range(augmentations_per_image):
#             # è·å–å¢å¼ºåçš„å›¾åƒåˆ—è¡¨ï¼ˆ1å¼ ä¼ ç»Ÿå¢å¼ºå›¾ï¼Œå¯é€‰1å¼ GANå¢å¼ºå›¾ï¼‰
#             aug_imgs = augmenter.augment(image)
#             # ä¿å­˜å¢å¼ºå›¾ï¼ˆå‘½åè§„åˆ™ï¼šåŸå›¾å_å¢å¼ºæ¬¡æ•°_ç±»å‹.jpgï¼‰
#             for img_type_idx, aug_img in enumerate(aug_imgs):
#                 img_type = "traditional" if img_type_idx == 0 else "gan"
#                 save_name = f"{img_file.stem}_aug{aug_idx+1}_{img_type}.jpg"
#                 save_path = save_dir / save_name
#                 aug_img.save(save_path, quality=95)  # ä¿å­˜ä¸ºJPGï¼Œè´¨é‡95

#         # æ¯å¤„ç†10%çš„å›¾åƒæ‰“å°è¿›åº¦
#         if (img_idx % max(1, total_images // 10)) == 0:
#             progress = (img_idx / total_images) * 100
#             logger.info(f"æ•°æ®å¢å¼ºè¿›åº¦ï¼š{img_idx}/{total_images}å¼ ï¼ˆ{progress:.1f}%ï¼‰")

#     # å¢å¼ºå®Œæˆ
#     logger.info(f"æ•°æ®å¢å¼ºå…¨éƒ¨å®Œæˆï¼å¢å¼ºåå›¾åƒä¿å­˜è‡³ï¼š{output_dir}")

# #74.58 æœ€å¥½çš„Augmentation
# import os 
# import random
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# from torchvision import transforms
# import albumentations as A
# from albumentations.pytorch import ToTensorV2
# import cv2
# from PIL import Image
# from pathlib import Path
# import logging
# from tqdm import tqdm

# # é…ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(name)s-%(levelname)s-%(message)s')
# logger = logging.getLogger(__name__)

# # è®¾å¤‡è®¾ç½®ï¼ˆä¼˜å…ˆGPUâ†’MPSâ†’CPUï¼‰
# def get_device():
#     if torch.cuda.is_available():
#         return torch.device("cuda")
#     elif torch.backends.mps.is_available():
#         return torch.device("mps")
#     else:
#         return torch.device("cpu")

# device = get_device()
# logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")


# # ===================== 1. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆMixup/CutMixï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# def mixup_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size = x.size(0)
#     index = torch.randperm(batch_size).to(x.device)
#     mixed_x = lam * x + (1 - lam) * x[index, :]
#     if y is None:
#         return mixed_x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return mixed_x, y_a, y_b, lam

# def cutmix_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size, _, H, W = x.size()
#     cut_rat = np.sqrt(1. - lam)
#     cut_w = int(W * cut_rat)
#     cut_h = int(H * cut_rat)
#     cx = np.random.randint(W)
#     cy = np.random.randint(H)
#     bbx1 = np.clip(cx - cut_w // 2, 0, W)
#     bby1 = np.clip(cy - cut_h // 2, 0, H)
#     bbx2 = np.clip(cx + cut_w // 2, 0, W)
#     bby2 = np.clip(cy + cut_h // 2, 0, H)
#     index = torch.randperm(batch_size).to(x.device)
#     x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
#     lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (H * W))
#     if y is None:
#         return x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return x, y_a, y_b, lam


# # ===================== 2. æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒä¼ ç»Ÿ/è‡ªåŠ¨åŒ–å¢å¼ºï¼‰ã€æ— ä¿®æ”¹ã€‘ =====================
# class ImageDataset(Dataset):
#     def __init__(
#         self, 
#         root_dir, 
#         img_size=(32, 32), 
#         use_auto_aug=False,
#         use_rand_aug=False,
#         is_train=True
#     ):
#         self.root_dir = root_dir
#         self.img_size = img_size
#         self.is_train = is_train
#         self.image_paths = [
#             p for p in Path(root_dir).glob('**/*') 
#             if p.suffix.lower() in ('.png', '.jpg', '.jpeg', '.bmp')
#         ]
#         logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.image_paths)} å¼ å›¾åƒï¼ˆè®­ç»ƒæ¨¡å¼ï¼š{is_train}ï¼‰")
#         self.transform = self._build_transform(use_auto_aug, use_rand_aug)

#     def _build_transform(self, use_auto_aug, use_rand_aug):
#         transform_list = []
#         if self.is_train:
#             transform_list.extend([
#                 transforms.RandomResizedCrop(size=self.img_size, scale=(0.08, 1.0), ratio=(3/4, 4/3)),
#                 transforms.RandomHorizontalFlip(p=0.5),
#                 transforms.RandomVerticalFlip(p=0.2),
#                 transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#                 transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#                 transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#                 transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 3.0))], p=0.2),
#                 transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#                 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)
#             ])
#         transform_list.extend([
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ])
#         return transforms.Compose(transform_list)

#     def __len__(self):
#         return len(self.image_paths)

#     def __getitem__(self, idx):
#         img_path = self.image_paths[idx]
#         try:
#             image = Image.open(img_path).convert('RGB')
#             return self.transform(image)
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æŸåå›¾åƒ {img_path}ï¼š{e}")
#             return torch.randn(3, self.img_size[0], self.img_size[1])


# # ===================== 3. GANæ¨¡å‹å®šä¹‰ï¼ˆå½»åº•ç§»é™¤å™ªå£°ç›¸å…³ï¼‰ã€æ ¸å¿ƒä¿®æ”¹ã€‘ =====================
# class SelfAttention(nn.Module):
#     """ä¿ç•™è‡ªæ³¨æ„åŠ›ï¼Œæ— ä¿®æ”¹"""
#     def __init__(self, in_dim):
#         super().__init__()
#         self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.key_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.value_conv = nn.Conv2d(in_dim, in_dim, 1)
#         self.gamma = nn.Parameter(torch.zeros(1))
#         self.softmax = nn.Softmax(dim=-1)

#     def forward(self, x):
#         batch_size, C, w, h = x.size()
#         proj_query = self.query_conv(x).view(batch_size, -1, w*h).permute(0, 2, 1)
#         proj_key = self.key_conv(x).view(batch_size, -1, w*h)
#         energy = torch.bmm(proj_query, proj_key)
#         attention = self.softmax(energy)
#         proj_value = self.value_conv(x).view(batch_size, -1, w*h)
#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))
#         out = out.view(batch_size, C, w, h)
#         return self.gamma * out + x


# class Generator(nn.Module):
#     """ã€ä¿®æ”¹1ã€‘ç§»é™¤latent_dimå’Œå™ªå£°è¾“å…¥ï¼Œæ”¹ä¸ºå›ºå®šç»´åº¦çš„è¾“å…¥å‘é‡"""
#     def __init__(self, channels=3, img_size=32, input_dim=100):
#         # ç”¨input_dimï¼ˆå›ºå®šè¾“å…¥ç»´åº¦ï¼‰æ›¿ä»£latent_dimï¼Œä¸å†ä¾èµ–å¤–éƒ¨å™ªå£°
#         super().__init__()
#         self.init_size = img_size // 4
#         self.input_dim = input_dim  # å›ºå®šè¾“å…¥ç»´åº¦ï¼ˆæ›¿ä»£å™ªå£°ç»´åº¦ï¼‰
#         # ã€ä¿®æ”¹2ã€‘çº¿æ€§å±‚è¾“å…¥ç»´åº¦æ”¹ä¸ºinput_dimï¼ˆæ— å™ªå£°ï¼Œä»…ç”¨å›ºå®šç»´åº¦å‘é‡ï¼‰
#         self.l1 = nn.Sequential(nn.Linear(self.input_dim, 128 * self.init_size ** 2))

#         self.conv_blocks = nn.Sequential(
#             nn.BatchNorm2d(128),
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 128, 3, 1, 1),
#             nn.BatchNorm2d(128, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 64, 3, 1, 1),
#             nn.BatchNorm2d(64, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             SelfAttention(64),
            
#             nn.Conv2d(64, channels, 3, 1, 1),
#             nn.Tanh()
#         )

#     def forward(self):
#         """ã€ä¿®æ”¹3ã€‘æ— è¾“å…¥å‚æ•°ï¼Œå†…éƒ¨ç”Ÿæˆå›ºå®šç»´åº¦çš„éšæœºå‘é‡ï¼ˆæ›¿ä»£å¤–éƒ¨å™ªå£°zï¼‰"""
#         # å†…éƒ¨ç”Ÿæˆéšæœºå‘é‡ï¼ˆä»…ç”¨äºæ¨¡å‹å‰å‘ï¼Œæ— å¤–éƒ¨å™ªå£°ä¾èµ–ï¼‰
#         x = torch.randn(1, self.input_dim, device=device)  # å•æ ·æœ¬ç”Ÿæˆ
#         out = self.l1(x)
#         out = out.view(out.shape[0], 128, self.init_size, self.init_size)
#         return self.conv_blocks(out)


# class Discriminator(nn.Module):
#     """ã€æ— ä¿®æ”¹ã€‘åˆ¤åˆ«å™¨ä¸æ¶‰åŠå™ªå£°ï¼Œä¿æŒåŸç»“æ„"""
#     def __init__(self, channels=3, img_size=32):
#         super().__init__()
#         def discriminator_block(in_filters, out_filters, bn=True):
#             block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True)]
#             if bn:
#                 block.append(nn.BatchNorm2d(out_filters, 0.8))
#             return block

#         self.model = nn.Sequential(
#             *discriminator_block(channels, 16, bn=False),
#             *discriminator_block(16, 32),
#             *discriminator_block(32, 64),
#             *discriminator_block(64, 128),
#         )

#         ds_size = img_size // (2**4)
#         self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size**2, 1), nn.Sigmoid())

#     def forward(self, img):
#         out = self.model(img)
#         out = out.view(out.shape[0], -1)
#         return self.adv_layer(out)


# # ===================== 4. GANè®­ç»ƒå™¨ï¼ˆå½»åº•ç§»é™¤å™ªå£°ç›¸å…³ï¼‰ã€æ ¸å¿ƒä¿®æ”¹ã€‘ =====================
# class GANTrainer:
#     def __init__(
#         self,
#         data_dir,
#         img_size=(32, 32),
#         epochs=30,
#         batch_size=64,
#         lr=0.0002,
#         weight_path="generator_weights.pth",
#         use_mixup=True,
#         use_cutmix=True,
#         generator_input_dim=100  # ã€æ–°å¢ã€‘ç”Ÿæˆå™¨å›ºå®šè¾“å…¥ç»´åº¦ï¼ˆæ›¿ä»£latent_dimï¼‰
#     ):
#         self.data_dir = data_dir
#         self.img_size = img_size
#         self.epochs = epochs
#         self.batch_size = batch_size
#         self.lr = lr
#         self.weight_path = weight_path
#         self.use_mixup = use_mixup
#         self.use_cutmix = use_cutmix
#         self.generator_input_dim = generator_input_dim

#         # ã€ä¿®æ”¹1ã€‘åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼šæ— latent_dimï¼Œä¼ generator_input_dim
#         self.generator = Generator(
#             img_size=img_size[0],
#             input_dim=self.generator_input_dim
#         ).to(device)
#         self.discriminator = Discriminator(img_size=img_size[0]).to(device)

#         # æŸå¤±ä¸ä¼˜åŒ–å™¨ï¼ˆæ— ä¿®æ”¹ï¼‰
#         self.adversarial_loss = nn.BCELoss().to(device)
#         self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))
#         self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))

#         # åŠ è½½æ•°æ®é›†ï¼ˆæ— ä¿®æ”¹ï¼‰
#         self.dataset = ImageDataset(
#             root_dir=data_dir,
#             img_size=img_size,
#             use_rand_aug=True,
#             is_train=True
#         )
#         self.dataloader = DataLoader(
#             self.dataset,
#             batch_size=batch_size,
#             shuffle=True,
#             num_workers=2,
#             pin_memory=True
#         )

#     def train(self):
#         logger.info(f"å¼€å§‹GANè®­ç»ƒï¼ˆMixup: {self.use_mixup}, CutMix: {self.use_cutmix}ï¼‰ï¼Œå…± {self.epochs} è½®")
        
#         for epoch in range(self.epochs):
#             pbar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.epochs}")
#             for imgs in pbar:
#                 batch_size = imgs.size(0)
#                 valid = torch.ones(batch_size, 1).to(device)
#                 fake = torch.zeros(batch_size, 1).to(device)
#                 real_imgs = imgs.to(device)

#                 # Mixup/CutMixå¤„ç†ï¼ˆæ— ä¿®æ”¹ï¼‰
#                 if self.use_mixup:
#                     real_imgs, _, _, _ = mixup_data(real_imgs, None, alpha=0.1)
#                 if self.use_cutmix and not self.use_mixup:
#                     real_imgs, _, _, _ = cutmix_data(real_imgs, None, alpha=0.1)

#                 # ----------------- è®­ç»ƒç”Ÿæˆå™¨ã€ä¿®æ”¹2ã€‘ç§»é™¤å™ªå£°zï¼Œç›´æ¥è°ƒç”¨generator() -----------------
#                 self.optimizer_G.zero_grad()
#                 # æ— å¤–éƒ¨å™ªå£°ï¼Œç”Ÿæˆå™¨å†…éƒ¨ç”Ÿæˆè¾“å…¥å‘é‡
#                 gen_imgs = torch.cat([self.generator() for _ in range(batch_size)], dim=0)  # æ‰¹é‡ç”Ÿæˆ
#                 g_loss = self.adversarial_loss(self.discriminator(gen_imgs), valid)
#                 g_loss.backward()
#                 self.optimizer_G.step()

#                 # ----------------- è®­ç»ƒåˆ¤åˆ«å™¨ï¼ˆæ— ä¿®æ”¹ï¼‰ -----------------
#                 self.optimizer_D.zero_grad()
#                 real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)
#                 fake_loss = self.adversarial_loss(self.discriminator(gen_imgs.detach()), fake)
#                 d_loss = (real_loss + fake_loss) / 2
#                 d_loss.backward()
#                 self.optimizer_D.step()

#                 pbar.set_postfix({"DæŸå¤±": d_loss.item(), "GæŸå¤±": g_loss.item()})

#             # ä¿å­˜æƒé‡ï¼ˆæ— ä¿®æ”¹ï¼‰
#             if (epoch + 1) % 10 == 0:
#                 torch.save({
#                     "generator_state_dict": self.generator.state_dict(),
#                     "input_dim": self.generator_input_dim  # ä¿å­˜è¾“å…¥ç»´åº¦ï¼Œåç»­åŠ è½½ç”¨
#                 }, f"generator_weights_epoch_{epoch+1}.pth")
#                 logger.info(f"å·²ä¿å­˜ç¬¬ {epoch+1} è½®GANæƒé‡")

#         torch.save({
#             "generator_state_dict": self.generator.state_dict(),
#             "input_dim": self.generator_input_dim
#         }, self.weight_path)
#         logger.info(f"GANè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæƒé‡ä¿å­˜è‡³ {self.weight_path}")


# # ===================== 5. å®Œæ•´å¢å¼ºå™¨ï¼ˆå½»åº•ç§»é™¤å™ªå£°ç›¸å…³ï¼‰ã€æ ¸å¿ƒä¿®æ”¹ã€‘ =====================
# class FullAugmenter:
#     def __init__(
#         self,
#         img_size=(32, 32),
#         use_auto_aug=False,
#         use_rand_aug=True,
#         gan_weight_path="generator_weights.pth",
#         use_gan=True
#     ):
#         self.img_size = img_size
#         self.use_gan = use_gan

#         # ä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼ºï¼ˆæ— ä¿®æ”¹ï¼‰
#         self.traditional_transform = self._build_traditional_transform(use_auto_aug, use_rand_aug)
#         self.inv_transform = transforms.Compose([
#             transforms.Normalize(mean=(-1.0, -1.0, -1.0), std=(2.0, 2.0, 2.0)),
#             transforms.ToPILImage()
#         ])

#         # ã€ä¿®æ”¹1ã€‘GANåˆå§‹åŒ–ï¼šç§»é™¤latent_dimï¼ŒåŠ è½½input_dim
#         self.gan_available = False
#         self.generator = None
#         self.generator_input_dim = 100  # é»˜è®¤è¾“å…¥ç»´åº¦
#         if use_gan:
#             self.gan_available = self._load_gan_weights(gan_weight_path)
#             if self.gan_available:
#                 self.gan_preprocess = transforms.Compose([
#                     transforms.Resize(img_size),
#                     transforms.ToTensor(),
#                     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#                 ])

#     def _build_traditional_transform(self, use_auto_aug, use_rand_aug):
#         """æ— ä¿®æ”¹ï¼Œä¿ç•™ä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼º"""
#         transform_list = [
#             transforms.RandomResizedCrop(size=self.img_size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),
#             transforms.RandomHorizontalFlip(p=0.5),
#             transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#             transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#             transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#             transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.2),
#             transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#             transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ]
#         return transforms.Compose(transform_list)

#     def _load_gan_weights(self, weight_path):
#         """ã€ä¿®æ”¹2ã€‘åŠ è½½æƒé‡æ—¶è¯»å–input_dimï¼Œæ— latent_dim"""
#         if not os.path.exists(weight_path):
#             logger.warning(f"æœªæ‰¾åˆ°GANæƒé‡æ–‡ä»¶ï¼š{weight_path}")
#             return False
#         try:
#             checkpoint = torch.load(weight_path, map_location=device)
#             self.generator_input_dim = checkpoint.get("input_dim", 100)
#             # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼ˆæ— latent_dimï¼Œä¼ input_dimï¼‰
#             self.generator = Generator(
#                 img_size=self.img_size[0],
#                 input_dim=self.generator_input_dim
#             ).to(device)
#             self.generator.load_state_dict(checkpoint["generator_state_dict"])
#             self.generator.eval()
#             logger.info(f"æˆåŠŸåŠ è½½GANæƒé‡ï¼š{weight_path}ï¼Œè¾“å…¥ç»´åº¦ï¼š{self.generator_input_dim}")
#             return True
#         except Exception as e:
#             logger.error(f"GANæƒé‡åŠ è½½å¤±è´¥ï¼š{e}")
#             return False

#     def traditional_augment(self, image: Image.Image) -> Image.Image:
#         """æ— ä¿®æ”¹ï¼Œä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼º"""
#         img_tensor = self.traditional_transform(image)
#         return self.inv_transform(img_tensor)

#     def gan_augment(self, image: Image.Image) -> Image.Image | None:
#         """ã€ä¿®æ”¹3ã€‘ç§»é™¤å™ªå£°zï¼Œç›´æ¥è°ƒç”¨generator()ç”Ÿæˆæ ·æœ¬"""
#         if not self.gan_available or self.generator is None:
#             return None
#         with torch.no_grad():
#             # æ— å¤–éƒ¨å™ªå£°ï¼Œç”Ÿæˆå™¨å†…éƒ¨ç”Ÿæˆæ ·æœ¬
#             gen_img = self.generator()  # å•æ ·æœ¬ç”Ÿæˆ
#             # èåˆåŸå§‹å›¾åƒç‰¹å¾ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
#             img_tensor = self.gan_preprocess(image).unsqueeze(0).to(device)
#             fused_img = 0.6 * gen_img + 0.4 * img_tensor
#             return self.inv_transform(fused_img.squeeze(0).cpu())

#     def augment(self, image: Image.Image, use_gan: bool = True) -> list[Image.Image]:
#         """æ— ä¿®æ”¹ï¼Œç»Ÿä¸€å¢å¼ºæ¥å£"""
#         aug_imgs = [self.traditional_augment(image)]
#         if use_gan and self.gan_available:
#             gan_img = self.gan_augment(image)
#             if gan_img:
#                 aug_imgs.append(gan_img)
#         return aug_imgs


# # ===================== 6. ä¸»æµç¨‹ï¼šGANè®­ç»ƒ+å…¨é‡æ•°æ®å¢å¼ºã€ä¿®æ”¹ï¼šç§»é™¤latent_dimå‚æ•°ã€‘ =====================
# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 10,
#     img_size: tuple = (32, 32),
#     train_gan: bool = True,
#     gan_epochs: int = 30,
#     use_auto_aug: bool = True,
#     use_rand_aug: bool = True,
#     use_gan: bool = True,
#     use_mixup_in_gan: bool = True,
#     generator_input_dim=100  # ã€æ–°å¢ã€‘ç”Ÿæˆå™¨è¾“å…¥ç»´åº¦ï¼ˆæ›¿ä»£latent_dimï¼‰
# ):
#     weight_path = "generator_weights.pth"

#     # æ­¥éª¤1ï¼šè®­ç»ƒGANï¼ˆæ— latent_dimï¼Œä¼ generator_input_dimï¼‰
#     if train_gan or not os.path.exists(weight_path):
#         trainer = GANTrainer(
#             data_dir=input_dir,
#             img_size=img_size,
#             epochs=gan_epochs,
#             use_mixup=use_mixup_in_gan,
#             weight_path=weight_path,
#             generator_input_dim=generator_input_dim
#         )
#         trainer.train()
#     else:
#         logger.info("æ£€æµ‹åˆ°å·²æœ‰GANæƒé‡ï¼Œè·³è¿‡è®­ç»ƒ")

#     # æ­¥éª¤2ï¼šåˆå§‹åŒ–å¢å¼ºå™¨ï¼ˆæ— ä¿®æ”¹ï¼‰
#     augmenter = FullAugmenter(
#         img_size=img_size,
#         use_auto_aug=use_auto_aug,
#         use_rand_aug=use_rand_aug,
#         gan_weight_path=weight_path,
#         use_gan=use_gan
#     )

#     # æ­¥éª¤3ï¼šæ‰¹é‡å¤„ç†å›¾åƒï¼ˆæ— ä¿®æ”¹ï¼‰
#     input_path = Path(input_dir)
#     output_path = Path(output_dir)
#     output_path.mkdir(parents=True, exist_ok=True)

#     image_extensions = ('.png', '.jpg', '.jpeg', '.bmp')
#     image_files = [f for f in input_path.rglob('*') if f.suffix.lower() in image_extensions]
#     logger.info(f"å…±æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒï¼Œå¼€å§‹å…¨é‡å¢å¼º...")

#     for img_path in image_files:
#         try:
#             image = Image.open(img_path).convert('RGB')
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æ— æ•ˆå›¾åƒ {img_path}ï¼š{e}")
#             continue

#         rel_dir = img_path.parent.relative_to(input_path)
#         target_dir = output_path / rel_dir
#         target_dir.mkdir(parents=True, exist_ok=True)

#         orig_path = target_dir / f"orig_{img_path.name}"
#         image.save(orig_path)

#         for i in range(augmentations_per_image):
#             use_gan_flag = use_gan and augmenter.gan_available and random.random() < 1
#             aug_imgs = augmenter.augment(image, use_gan=use_gan_flag)
            
#             for j, aug_img in enumerate(aug_imgs):
#                 if i * 2 + j >= augmentations_per_image:
#                     break
#                 aug_save_path = target_dir / f"aug_{i}_{j}_{img_path.name}"
#                 aug_img.save(aug_save_path)

#     logger.info(f"å…¨é‡æ•°æ®å¢å¼ºå®Œæˆï¼ç»“æœä¿å­˜è‡³ {output_dir}")

# import os 
# import random
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# from torchvision import transforms
# from PIL import Image
# from pathlib import Path
# import logging
# from tqdm import tqdm
# import subprocess
# import sys

# def install_package(package):
#     """åœ¨Pythonä»£ç ä¸­å®‰è£…æŒ‡å®šåŒ…"""
#     subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# # å®‰è£…diffusersåŠå…¶ä¾èµ–
# try:
#     # å…ˆå°è¯•å¯¼å…¥ï¼Œåˆ¤æ–­æ˜¯å¦å·²å®‰è£…
#     import diffusers
#     import transformers
#     import accelerate
#     print("diffusersåŠå…¶ä¾èµ–å·²å®‰è£…ï¼Œæ— éœ€é‡å¤å®‰è£…")
# except ImportError:
#     print("æ­£åœ¨å®‰è£…diffusersåŠå…¶ä¾èµ–...")
#     install_package("diffusers")
#     install_package("transformers")
#     install_package("accelerate")
#     print("å®‰è£…å®Œæˆ")

# # åç»­å¯ä»¥æ­£å¸¸ä½¿ç”¨diffusersç›¸å…³åŠŸèƒ½

# # -------------------------- Stable Diffusionä¾èµ– --------------------------
# from diffusers import StableDiffusionImg2ImgPipeline

# # é…ç½®æ—¥å¿—
# logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(name)s-%(levelname)s-%(message)s')
# logger = logging.getLogger(__name__)

# # è®¾å¤‡è®¾ç½®
# def get_device():
#     if torch.cuda.is_available():
#         return torch.device("cuda")
#     elif torch.backends.mps.is_available():
#         return torch.device("mps")
#     else:
#         return torch.device("cpu")

# device = get_device()
# logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")


# # ===================== 1. åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆMixup/CutMixï¼‰ =====================
# def mixup_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size = x.size(0)
#     index = torch.randperm(batch_size).to(x.device)
#     mixed_x = lam * x + (1 - lam) * x[index, :]
#     if y is None:
#         return mixed_x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return mixed_x, y_a, y_b, lam

# def cutmix_data(x, y, alpha=0.2):
#     if alpha <= 0 or not isinstance(x, torch.Tensor):
#         return x, y, None, None
#     lam = np.random.beta(alpha, alpha)
#     batch_size, _, H, W = x.size()
#     cut_rat = np.sqrt(1. - lam)
#     cut_w = int(W * cut_rat)
#     cut_h = int(H * cut_rat)
#     cx = np.random.randint(W)
#     cy = np.random.randint(H)
#     bbx1 = np.clip(cx - cut_w // 2, 0, W)
#     bby1 = np.clip(cy - cut_h // 2, 0, H)
#     bbx2 = np.clip(cx + cut_w // 2, 0, W)
#     bby2 = np.clip(cy + cut_h // 2, 0, H)
#     index = torch.randperm(batch_size).to(x.device)
#     x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
#     lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (H * W))
#     if y is None:
#         return x, None, None, lam
#     else:
#         y_a, y_b = y, y[index]
#         return x, y_a, y_b, lam


# # ===================== 2. æ•°æ®åŠ è½½å™¨ =====================
# class ImageDataset(Dataset):
#     def __init__(
#         self, 
#         root_dir, 
#         img_size=(32, 32), 
#         use_auto_aug=False,
#         use_rand_aug=False,
#         is_train=True
#     ):
#         self.root_dir = root_dir
#         self.img_size = img_size
#         self.is_train = is_train
#         self.image_paths = [
#             p for p in Path(root_dir).glob('**/*') 
#             if p.suffix.lower() in ('.png', '.jpg', '.jpeg', '.bmp')
#         ]
#         logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.image_paths)} å¼ å›¾åƒï¼ˆè®­ç»ƒæ¨¡å¼ï¼š{is_train}ï¼‰")
#         self.transform = self._build_transform(use_auto_aug, use_rand_aug)

#     def _build_transform(self, use_auto_aug, use_rand_aug):
#         transform_list = []
#         if self.is_train:
#             transform_list.extend([
#                 transforms.RandomResizedCrop(size=self.img_size, scale=(0.08, 1.0), ratio=(3/4, 4/3)),
#                 transforms.RandomHorizontalFlip(p=0.5),
#                 transforms.RandomVerticalFlip(p=0.2),
#                 transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#                 transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#                 transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#                 transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 3.0))], p=0.2),
#                 transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#                 transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)
#             ])
#         transform_list.extend([
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ])
#         return transforms.Compose(transform_list)

#     def __len__(self):
#         return len(self.image_paths)

#     def __getitem__(self, idx):
#         img_path = self.image_paths[idx]
#         try:
#             image = Image.open(img_path).convert('RGB')
#             return self.transform(image)
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æŸåå›¾åƒ {img_path}ï¼š{e}")
#             return torch.randn(3, self.img_size[0], self.img_size[1])


# # ===================== 3. GANæ¨¡å‹å®šä¹‰ =====================
# class SelfAttention(nn.Module):
#     def __init__(self, in_dim):
#         super().__init__()
#         self.query_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.key_conv = nn.Conv2d(in_dim, in_dim//8, 1)
#         self.value_conv = nn.Conv2d(in_dim, in_dim, 1)
#         self.gamma = nn.Parameter(torch.zeros(1))
#         self.softmax = nn.Softmax(dim=-1)

#     def forward(self, x):
#         batch_size, C, w, h = x.size()
#         proj_query = self.query_conv(x).view(batch_size, -1, w*h).permute(0, 2, 1)
#         proj_key = self.key_conv(x).view(batch_size, -1, w*h)
#         energy = torch.bmm(proj_query, proj_key)
#         attention = self.softmax(energy)
#         proj_value = self.value_conv(x).view(batch_size, -1, w*h)
#         out = torch.bmm(proj_value, attention.permute(0, 2, 1))
#         out = out.view(batch_size, C, w, h)
#         return self.gamma * out + x


# class Generator(nn.Module):
#     def __init__(self, channels=3, img_size=32, input_dim=100):
#         super().__init__()
#         self.init_size = img_size // 4
#         self.input_dim = input_dim
#         self.l1 = nn.Sequential(nn.Linear(self.input_dim, 128 * self.init_size ** 2))

#         self.conv_blocks = nn.Sequential(
#             nn.BatchNorm2d(128),
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 128, 3, 1, 1),
#             nn.BatchNorm2d(128, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             nn.Upsample(scale_factor=2),
#             nn.Conv2d(128, 64, 3, 1, 1),
#             nn.BatchNorm2d(64, 0.8),
#             nn.LeakyReLU(0.2, inplace=True),
            
#             SelfAttention(64),
            
#             nn.Conv2d(64, channels, 3, 1, 1),
#             nn.Tanh()
#         )

#     def forward(self):
#         x = torch.randn(1, self.input_dim, device=device)
#         out = self.l1(x)
#         out = out.view(out.shape[0], 128, self.init_size, self.init_size)
#         return self.conv_blocks(out)


# class Discriminator(nn.Module):
#     def __init__(self, channels=3, img_size=32):
#         super().__init__()
#         def discriminator_block(in_filters, out_filters, bn=True):
#             block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True)]
#             if bn:
#                 block.append(nn.BatchNorm2d(out_filters, 0.8))
#             return block

#         self.model = nn.Sequential(
#             *discriminator_block(channels, 16, bn=False),
#             *discriminator_block(16, 32),
#             *discriminator_block(32, 64),
#             *discriminator_block(64, 128),
#         )

#         ds_size = img_size // (2**4)
#         self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size**2, 1), nn.Sigmoid())

#     def forward(self, img):
#         out = self.model(img)
#         out = out.view(out.shape[0], -1)
#         return self.adv_layer(out)


# # ===================== 4. GANè®­ç»ƒå™¨ =====================
# class GANTrainer:
#     def __init__(
#         self,
#         data_dir,
#         img_size=(32, 32),
#         epochs=30,
#         batch_size=64,
#         lr=0.0002,
#         weight_path="generator_weights.pth",
#         use_mixup=True,
#         use_cutmix=True,
#         generator_input_dim=100
#     ):
#         self.data_dir = data_dir
#         self.img_size = img_size
#         self.epochs = epochs
#         self.batch_size = batch_size
#         self.lr = lr
#         self.weight_path = weight_path
#         self.use_mixup = use_mixup
#         self.use_cutmix = use_cutmix
#         self.generator_input_dim = generator_input_dim

#         self.generator = Generator(
#             img_size=img_size[0],
#             input_dim=self.generator_input_dim
#         ).to(device)
#         self.discriminator = Discriminator(img_size=img_size[0]).to(device)

#         self.adversarial_loss = nn.BCELoss().to(device)
#         self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))
#         self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))

#         self.dataset = ImageDataset(
#             root_dir=data_dir,
#             img_size=img_size,
#             use_rand_aug=True,
#             is_train=True
#         )
#         self.dataloader = DataLoader(
#             self.dataset,
#             batch_size=batch_size,
#             shuffle=True,
#             num_workers=2,
#             pin_memory=True
#         )

#     def train(self):
#         logger.info(f"å¼€å§‹GANè®­ç»ƒï¼ˆMixup: {self.use_mixup}, CutMix: {self.use_cutmix}ï¼‰ï¼Œå…± {self.epochs} è½®")
        
#         for epoch in range(self.epochs):
#             pbar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.epochs}")
#             for imgs in pbar:
#                 batch_size = imgs.size(0)
#                 valid = torch.ones(batch_size, 1).to(device)
#                 fake = torch.zeros(batch_size, 1).to(device)
#                 real_imgs = imgs.to(device)

#                 if self.use_mixup:
#                     real_imgs, _, _, _ = mixup_data(real_imgs, None, alpha=0.1)
#                 if self.use_cutmix and not self.use_mixup:
#                     real_imgs, _, _, _ = cutmix_data(real_imgs, None, alpha=0.1)

#                 # è®­ç»ƒç”Ÿæˆå™¨
#                 self.optimizer_G.zero_grad()
#                 gen_imgs = torch.cat([self.generator() for _ in range(batch_size)], dim=0)
#                 g_loss = self.adversarial_loss(self.discriminator(gen_imgs), valid)
#                 g_loss.backward()
#                 self.optimizer_G.step()

#                 # è®­ç»ƒåˆ¤åˆ«å™¨
#                 self.optimizer_D.zero_grad()
#                 real_loss = self.adversarial_loss(self.discriminator(real_imgs), valid)
#                 fake_loss = self.adversarial_loss(self.discriminator(gen_imgs.detach()), fake)
#                 d_loss = (real_loss + fake_loss) / 2
#                 d_loss.backward()
#                 self.optimizer_D.step()

#                 pbar.set_postfix({"DæŸå¤±": d_loss.item(), "GæŸå¤±": g_loss.item()})

#             if (epoch + 1) % 10 == 0:
#                 torch.save({
#                     "generator_state_dict": self.generator.state_dict(),
#                     "input_dim": self.generator_input_dim
#                 }, f"generator_weights_epoch_{epoch+1}.pth")
#                 logger.info(f"å·²ä¿å­˜ç¬¬ {epoch+1} è½®GANæƒé‡")

#         torch.save({
#             "generator_state_dict": self.generator.state_dict(),
#             "input_dim": self.generator_input_dim
#         }, self.weight_path)
#         logger.info(f"GANè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæƒé‡ä¿å­˜è‡³ {self.weight_path}")


# # ===================== 5. å®Œæ•´å¢å¼ºå™¨ï¼ˆå«Stable Diffusionï¼‰ =====================
# class FullAugmenter:
#     def __init__(
#         self,
#         img_size=(32, 32),
#         use_auto_aug=False,
#         use_rand_aug=True,
#         gan_weight_path="generator_weights.pth",
#         use_gan=True,
#         # Stable Diffusionå‚æ•°
#         use_sd=True,
#         sd_model_name="CompVis/stable-diffusion-v1-4",
#         sd_guidance_scale=7.5,
#         sd_strength=0.3,
#         sd_num_inference_steps=20
#     ):
#         self.img_size = img_size
#         self.use_gan = use_gan
        
#         # Stable Diffusionåˆå§‹åŒ–
#         self.use_sd = use_sd
#         self.sd_guidance_scale = sd_guidance_scale
#         self.sd_strength = sd_strength
#         self.sd_num_inference_steps = sd_num_inference_steps
#         self.sd_pipeline = None
#         self.sd_available = self._init_stable_diffusion(sd_model_name)

#         # ä¼ ç»Ÿå¢å¼º
#         self.traditional_transform = self._build_traditional_transform(use_auto_aug, use_rand_aug)
#         self.inv_transform = transforms.Compose([
#             transforms.Normalize(mean=(-1.0, -1.0, -1.0), std=(2.0, 2.0, 2.0)),
#             transforms.ToPILImage()
#         ])

#         # GANåˆå§‹åŒ–
#         self.gan_available = False
#         self.generator = None
#         self.generator_input_dim = 100
#         if use_gan:
#             self.gan_available = self._load_gan_weights(gan_weight_path)
#             if self.gan_available:
#                 self.gan_preprocess = transforms.Compose([
#                     transforms.Resize(img_size),
#                     transforms.ToTensor(),
#                     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#                 ])

#     def _init_stable_diffusion(self, model_name):
#         """åˆå§‹åŒ–Stable Diffusionç®¡é“"""
#         if not self.use_sd:
#             logger.info("æœªå¯ç”¨Stable Diffusionå¢å¼º")
#             return False
#         try:
#             # åŠ è½½SDæ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½è‡³ç¼“å­˜ï¼‰
#             self.sd_pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
#                 model_name,
#                 torch_dtype=torch.float16 if device.type == "cuda" else torch.float32
#             ).to(device)
#             # å¯ç”¨å®‰å…¨æ£€æŸ¥å™¨ï¼ˆå¯é€‰ï¼Œé¿å…ç”Ÿæˆä¸å½“å†…å®¹ï¼‰
#             self.sd_pipeline.safety_checker = lambda images, **kwargs: (images, [False]*len(images))
#             logger.info(f"æˆåŠŸåŠ è½½Stable Diffusionæ¨¡å‹ï¼š{model_name}")
#             return True
#         except Exception as e:
#             logger.error(f"Stable Diffusionåˆå§‹åŒ–å¤±è´¥ï¼š{e}")
#             return False

#     def _load_gan_weights(self, weight_path):
#         """åŠ è½½GANæƒé‡"""
#         if not os.path.exists(weight_path):
#             logger.warning(f"æœªæ‰¾åˆ°GANæƒé‡æ–‡ä»¶ï¼š{weight_path}")
#             return False
#         try:
#             checkpoint = torch.load(weight_path, map_location=device)
#             self.generator_input_dim = checkpoint.get("input_dim", 100)
#             self.generator = Generator(
#                 img_size=self.img_size[0],
#                 input_dim=self.generator_input_dim
#             ).to(device)
#             self.generator.load_state_dict(checkpoint["generator_state_dict"])
#             self.generator.eval()
#             logger.info(f"æˆåŠŸåŠ è½½GANæƒé‡ï¼š{weight_path}")
#             return True
#         except Exception as e:
#             logger.error(f"GANæƒé‡åŠ è½½å¤±è´¥ï¼š{e}")
#             return False

#     def _build_traditional_transform(self, use_auto_aug, use_rand_aug):
#         """ä¼ ç»Ÿ+è‡ªåŠ¨åŒ–å¢å¼º"""
#         transform_list = [
#             transforms.RandomResizedCrop(size=self.img_size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),
#             transforms.RandomHorizontalFlip(p=0.5),
#             transforms.RandomRotation(degrees=(-15, 15), fill=(255, 255, 255)),
#             transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, fill=(255, 255, 255)),
#             transforms.RandAugment(num_ops=2, magnitude=9) if use_rand_aug and not use_auto_aug else transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET) if use_auto_aug else transforms.Lambda(lambda x: x),
#             transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.2),
#             transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.1),
#             transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#             transforms.Resize(self.img_size),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
#         ]
#         return transforms.Compose(transform_list)

#     def traditional_augment(self, image: Image.Image) -> Image.Image:
#         """ä¼ ç»Ÿå¢å¼º"""
#         img_tensor = self.traditional_transform(image)
#         return self.inv_transform(img_tensor)

#     def gan_augment(self, image: Image.Image) -> Image.Image | None:
#         """GANå¢å¼º"""
#         if not self.gan_available or self.generator is None:
#             return None
#         with torch.no_grad():
#             gen_img = self.generator()
#             img_tensor = self.gan_preprocess(image).unsqueeze(0).to(device)
#             fused_img = 0.6 * gen_img + 0.4 * img_tensor
#             return self.inv_transform(fused_img.squeeze(0).cpu())

#     def sd_augment(self, image: Image.Image) -> Image.Image | None:
#         """Stable Diffusionå¢å¼ºï¼ˆimg2imgï¼‰"""
#         if not self.sd_available or self.sd_pipeline is None:
#             return None
#         try:
#             # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥é€‚é…SDï¼ˆSDå¯¹512x512æ•ˆæœæœ€ä½³ï¼‰
#             sd_img_size = (512, 512)
#             resized_img = image.resize(sd_img_size, Image.LANCZOS)
            
#             # ç”Ÿæˆæç¤ºè¯ï¼ˆå¯æ ¹æ®æ•°æ®é›†ç‰¹æ€§ä¿®æ”¹ï¼‰
#             prompt = "high quality, clear details, consistent style"
            
#             # è°ƒç”¨img2imgç”Ÿæˆå¢å¼ºå›¾åƒ
#             with torch.no_grad():
#                 gen_images = self.sd_pipeline(
#                     prompt=prompt,
#                     image=resized_img,
#                     strength=self.sd_strength,
#                     guidance_scale=self.sd_guidance_scale,
#                     num_inference_steps=self.sd_num_inference_steps
#                 ).images
            
#             # ç¼©æ”¾å›ç›®æ ‡å°ºå¯¸
#             return gen_images[0].resize(self.img_size, Image.LANCZOS)
#         except Exception as e:
#             logger.warning(f"Stable Diffusionå¢å¼ºå¤±è´¥ï¼š{e}")
#             return None

#     def augment(self, image: Image.Image, use_gan: bool = True, use_sd: bool = True) -> list[Image.Image]:
#         """ç»¼åˆå¢å¼ºæ¥å£"""
#         aug_imgs = [self.traditional_augment(image)]
        
#         # æ·»åŠ GANå¢å¼º
#         if use_gan and self.gan_available and random.random() < 0.5:
#             gan_img = self.gan_augment(image)
#             if gan_img:
#                 aug_imgs.append(gan_img)
        
#         # æ·»åŠ Stable Diffusionå¢å¼º
#         if use_sd and self.sd_available and random.random() < 0.3:  # è¾ƒä½æ¦‚ç‡ï¼Œé¿å…ç”Ÿæˆè¿‡å¤š
#             sd_img = self.sd_augment(image)
#             if sd_img:
#                 aug_imgs.append(sd_img)
        
#         return aug_imgs


# # ===================== 6. ä¸»æµç¨‹ï¼šGANè®­ç»ƒ+å…¨é‡æ•°æ®å¢å¼º =====================
# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 10,
#     img_size: tuple = (32, 32),
#     train_gan: bool = True,
#     gan_epochs: int = 30,
#     use_auto_aug: bool = True,
#     use_rand_aug: bool = True,
#     use_gan: bool = True,
#     use_sd: bool = True,
#     use_mixup_in_gan: bool = True,
#     generator_input_dim=100
# ):
#     weight_path = "generator_weights.pth"

#     # æ­¥éª¤1ï¼šè®­ç»ƒGAN
#     if train_gan or not os.path.exists(weight_path):
#         trainer = GANTrainer(
#             data_dir=input_dir,
#             img_size=img_size,
#             epochs=gan_epochs,
#             use_mixup=use_mixup_in_gan,
#             weight_path=weight_path,
#             generator_input_dim=generator_input_dim
#         )
#         trainer.train()
#     else:
#         logger.info("æ£€æµ‹åˆ°å·²æœ‰GANæƒé‡ï¼Œè·³è¿‡è®­ç»ƒ")

#     # æ­¥éª¤2ï¼šåˆå§‹åŒ–å¢å¼ºå™¨
#     augmenter = FullAugmenter(
#         img_size=img_size,
#         use_auto_aug=use_auto_aug,
#         use_rand_aug=use_rand_aug,
#         gan_weight_path=weight_path,
#         use_gan=use_gan,
#         use_sd=use_sd
#     )

#     # æ­¥éª¤3ï¼šæ‰¹é‡å¤„ç†å›¾åƒ
#     input_path = Path(input_dir)
#     output_path = Path(output_dir)
#     output_path.mkdir(parents=True, exist_ok=True)

#     image_extensions = ('.png', '.jpg', '.jpeg', '.bmp')
#     image_files = [f for f in input_path.rglob('*') if f.suffix.lower() in image_extensions]
#     logger.info(f"å…±æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒï¼Œå¼€å§‹å…¨é‡å¢å¼º...")

#     for img_path in image_files:
#         try:
#             image = Image.open(img_path).convert('RGB')
#         except Exception as e:
#             logger.warning(f"è·³è¿‡æ— æ•ˆå›¾åƒ {img_path}ï¼š{e}")
#             continue

#         rel_dir = img_path.parent.relative_to(input_path)
#         target_dir = output_path / rel_dir
#         target_dir.mkdir(parents=True, exist_ok=True)

#         # ä¿å­˜åŸå›¾
#         orig_path = target_dir / f"orig_{img_path.name}"
#         image.save(orig_path)

#         # ç”Ÿæˆå¢å¼ºå›¾åƒ
#         for i in range(augmentations_per_image):
#             aug_imgs = augmenter.augment(image, use_gan=use_gan, use_sd=use_sd)
#             for j, aug_img in enumerate(aug_imgs):
#                 if i * len(aug_imgs) + j >= augmentations_per_image:
#                     break
#                 aug_save_path = target_dir / f"aug_{i}_{j}_{img_path.name}"
#                 aug_img.save(aug_save_path)

#     logger.info(f"å…¨é‡æ•°æ®å¢å¼ºå®Œæˆï¼ç»“æœä¿å­˜è‡³ {output_dir}")
# import torch
# import torchvision.transforms as transforms
# from torchvision.transforms import autoaugment, transforms
# import numpy as np
# import random
# import os
# from PIL import Image
# import torchvision.transforms.functional as F

# class CIFAR100AdvancedAugmentation:
#     def __init__(self):
#         # è®­ç»ƒé›†å¢å¼º - å¼ºå¢å¼ºç­–ç•¥
#         self.train_transform = transforms.Compose([
#             transforms.Resize(72),
#             transforms.RandomCrop(64, padding=4),
#             transforms.RandomHorizontalFlip(p=0.5),
#             transforms.RandomVerticalFlip(p=0.2),
#             transforms.ColorJitter(
#                 brightness=0.4,
#                 contrast=0.4, 
#                 saturation=0.4,
#                 hue=0.1
#             ),
#             transforms.RandomRotation(15),
#             transforms.RandomGrayscale(p=0.1),
#             transforms.ToTensor(),
#             transforms.Normalize(
#                 mean=[0.5071, 0.4867, 0.4408],
#                 std=[0.2675, 0.2565, 0.2761]
#             ),
#             transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3))
#         ])
        
#         # éªŒè¯é›†å¢å¼º - ä»…åŸºç¡€é¢„å¤„ç†
#         self.val_transform = transforms.Compose([
#             transforms.Resize(64),
#             transforms.ToTensor(),
#             transforms.Normalize(
#                 mean=[0.5071, 0.4867, 0.4408],
#                 std=[0.2675, 0.2565, 0.2761]
#             )
#         ])
        
#         # æµ‹è¯•æ—¶å¢å¼º (TTA)
#         self.tta_transform = transforms.Compose([
#             transforms.Resize(64),
#             transforms.ToTensor(),
#             transforms.Normalize(
#                 mean=[0.5071, 0.4867, 0.4408],
#                 std=[0.2675, 0.2565, 0.2761]
#             )
#         ])

#     def get_train_transform(self):
#         return self.train_transform

#     def get_val_transform(self):
#         return self.val_transform

#     def get_tta_transforms(self):
#         """è¿”å›æµ‹è¯•æ—¶å¢å¼ºçš„å˜æ¢åˆ—è¡¨"""
#         tta_transforms = []
        
#         # åŸå§‹å›¾åƒ
#         tta_transforms.append(transforms.Compose([
#             transforms.Resize(64),
#             transforms.ToTensor(),
#             transforms.Normalize(
#                 mean=[0.5071, 0.4867, 0.4408],
#                 std=[0.2675, 0.2565, 0.2761]
#             )
#         ]))
        
#         # æ°´å¹³ç¿»è½¬
#         tta_transforms.append(transforms.Compose([
#             transforms.Resize(64),
#             transforms.RandomHorizontalFlip(p=1.0),
#             transforms.ToTensor(),
#             transforms.Normalize(
#                 mean=[0.5071, 0.4867, 0.4408],
#                 std=[0.2675, 0.2565, 0.2761]
#             )
#         ]))
        
#         # ä¸­å¿ƒè£å‰ª
#         tta_transforms.append(transforms.Compose([
#             transforms.Resize(72),
#             transforms.CenterCrop(64),
#             transforms.ToTensor(),
#             transforms.Normalize(
#                 mean=[0.5071, 0.4867, 0.4408],
#                 std=[0.2675, 0.2565, 0.2761]
#             )
#         ]))
        
#         return tta_transforms

# class CutMix:
#     def __init__(self, alpha=1.0):
#         self.alpha = alpha

#     def __call__(self, batch):
#         images, labels = batch
#         indices = torch.randperm(images.size(0))
#         shuffled_images = images[indices]
#         shuffled_labels = labels[indices]
        
#         lam = np.random.beta(self.alpha, self.alpha)
#         bbx1, bby1, bbx2, bby2 = self.rand_bbox(images.size(), lam)
#         images[:, :, bbx1:bbx2, bby1:bby2] = shuffled_images[:, :, bbx1:bbx2, bby1:bby2]
#         lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size()[-1] * images.size()[-2]))
        
#         return images, labels, shuffled_labels, lam

#     def rand_bbox(self, size, lam):
#         W = size[2]
#         H = size[3]
#         cut_rat = np.sqrt(1. - lam)
#         cut_w = int(W * cut_rat)
#         cut_h = int(H * cut_rat)

#         cx = np.random.randint(W)
#         cy = np.random.randint(H)

#         bbx1 = np.clip(cx - cut_w // 2, 0, W)
#         bby1 = np.clip(cy - cut_h // 2, 0, H)
#         bbx2 = np.clip(cx + cut_w // 2, 0, W)
#         bby2 = np.clip(cy + cut_h // 2, 0, H)

#         return bbx1, bby1, bbx2, bby2

# class MixUp:
#     def __init__(self, alpha=1.0):
#         self.alpha = alpha

#     def __call__(self, batch):
#         images, labels = batch
#         indices = torch.randperm(images.size(0))
#         shuffled_images = images[indices]
#         shuffled_labels = labels[indices]
        
#         lam = np.random.beta(self.alpha, self.alpha)
#         images = lam * images + (1 - lam) * shuffled_images
        
#         return images, labels, shuffled_labels, lam

# def augment_dataset(input_dir, output_dir, augmentations_per_image=5):
#     """
#     å¯¹æ•°æ®é›†è¿›è¡Œå¢å¼ºï¼Œç”Ÿæˆå¢å¼ºåçš„å›¾åƒå¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•
#     """
#     # åˆ›å»ºè¾“å‡ºç›®å½•
#     os.makedirs(output_dir, exist_ok=True)
    
#     # å®šä¹‰å¢å¼ºå˜æ¢
#     augmentation_transforms = transforms.Compose([
#         transforms.RandomHorizontalFlip(p=0.5),
#         transforms.RandomRotation(degrees=15),
#         transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#         transforms.RandomResizedCrop(32, scale=(0.8, 1.0), ratio=(0.75, 1.33)),
#         transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
#     ])
    
#     # éå†æ¯ä¸ªç±»åˆ«ç›®å½•
#     for class_name in os.listdir(input_dir):
#         class_input_dir = os.path.join(input_dir, class_name)
#         class_output_dir = os.path.join(output_dir, class_name)
#         os.makedirs(class_output_dir, exist_ok=True)
        
#         # éå†ç±»åˆ«ç›®å½•ä¸­çš„æ¯ä¸ªå›¾åƒ
#         for img_name in os.listdir(class_input_dir):
#             img_path = os.path.join(class_input_dir, img_name)
            
#             # æ‰“å¼€å›¾åƒ
#             try:
#                 image = Image.open(img_path)
#             except Exception as e:
#                 print(f"Error opening image {img_path}: {e}")
#                 continue
            
#             # ä¿å­˜åŸå§‹å›¾åƒ
#             base_name = os.path.splitext(img_name)[0]
#             image.save(os.path.join(class_output_dir, f"{base_name}_0.jpg"))
            
#             # ç”Ÿæˆå¢å¼ºå›¾åƒ
#             for i in range(augmentations_per_image):
#                 augmented_image = augmentation_transforms(image)
#                 augmented_image.save(os.path.join(class_output_dir, f"{base_name}_{i+1}.jpg"))
    
#     print(f"Data augmentation completed. Augmented images saved to {output_dir}")

# def load_transforms():
#     """
#     è¿”å›ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„æ•°æ®å˜æ¢
#     """
#     augmentation = CIFAR100AdvancedAugmentation()
#     return {
#         'train': augmentation.get_train_transform(),
#         'val': augmentation.get_val_transform()
#     }
# import logging
# import random
# from pathlib import Path
# from typing import List, Tuple

# import numpy as np
# import torch
# import albumentations as A
# from PIL import Image

# # Configure logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)


# class ImageAugmenter:
#     """Class to handle image augmentation operations using Albumentations."""

#     def __init__(
#         self,
#         augmentations_per_image: int = 5,
#         seed: int = 42,
#         save_original: bool = True,
#         image_extensions: Tuple[str, ...] = (".png", ".jpg", ".jpeg"),
#     ):
#         """
#         Initialize the ImageAugmenter.

#         Args:
#             augmentations_per_image: Number of augmented versions per original image.
#             seed: Random seed for reproducibility.
#             save_original: Whether to save the original image with prefix 'orig_'.
#             image_extensions: Tuple of valid image file extensions.
#         """
#         self.augmentations_per_image = augmentations_per_image
#         self.seed = seed
#         self.save_original = save_original
#         self.image_extensions = image_extensions

#         self._set_seed()

#         # Define Albumentations pipeline
#         self.transform = A.Compose(
#             [
#                 A.Rotate(limit=15, p=0.8),
#                 A.HorizontalFlip(p=0.5),
#                 A.ShiftScaleRotate(
#                     shift_limit=0.1,
#                     scale_limit=0.1,
#                     rotate_limit=0,
#                     p=0.8,
#                     border_mode=0,  # cv2.BORDER_CONSTANT
#                 ),
#                 A.ColorJitter(
#                     brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.8
#                 ),
#                 A.OneOf(
#                     [
#                         A.GaussianBlur(blur_limit=(3, 7), p=0.5),
#                         A.MotionBlur(blur_limit=7, p=0.5),
#                     ],
#                     p=0.3,
#                 ),
#                 A.RandomBrightnessContrast(p=0.2),
#             ]
#         )

#     def _set_seed(self):
#         """Set random seeds for reproducibility."""
#         random.seed(self.seed)
#         np.random.seed(self.seed)
#         torch.manual_seed(self.seed)
#         if torch.cuda.is_available():
#             torch.cuda.manual_seed_all(self.seed)

#     def augment_image(self, image: Image.Image) -> Image.Image:
#         """
#         Apply augmentation transforms to a single image using Albumentations.

#         Args:
#             image: PIL Image to augment.

#         Returns:
#             Augmented PIL Image.
#         """
#         # Convert PIL to NumPy array (RGB)
#         image_np = np.array(image)

#         # Apply Albumentations transform
#         augmented = self.transform(image=image_np)
#         augmented_image_np = augmented["image"]

#         # Convert back to PIL Image
#         return Image.fromarray(augmented_image_np.astype(np.uint8))

#     def process_directory(self, input_dir: str, output_dir: str) -> None:
#         """
#         Augment all images in input directory and save to output directory.

#         Preserves folder structure. Skips files that fail to load.

#         Args:
#             input_dir: Path to input directory with class subfolders.
#             output_dir: Path to output directory for augmented images.
#         """
#         input_path = Path(input_dir)
#         output_path = Path(output_dir)
#         output_path.mkdir(parents=True, exist_ok=True)
#         count = 0

#         image_files = self._find_image_files(input_path)

#         logger.info(f"Found {len(image_files)} images to augment.")

#         for img_path in image_files:
#             try:
#                 image = Image.open(img_path).convert("RGB")
#             except Exception as e:
#                 logger.warning(f"Failed to load image {img_path}: {e}")
#                 continue

#             # Determine output subdirectory
#             rel_dir = img_path.parent.relative_to(input_path)
#             target_dir = output_path / rel_dir
#             if not target_dir.exists():
#                 target_dir.mkdir(parents=True, exist_ok=True)

#             # Save original if requested
#             if self.save_original:
#                 orig_name = f"orig_{img_path.name}"
#                 image.save(target_dir / orig_name)

#             # Generate and save augmented versions
#             for i in range(self.augmentations_per_image):
#                 augmented = self.augment_image(image.copy())
#                 aug_name = f"aug_{i}_{img_path.name}"
#                 augmented.save(target_dir / aug_name)
#                 count += 1

#         logger.info(
#             f"Augmentation of {count} images completed. Output saved to: {output_dir}"
#         )

#     def _find_image_files(self, root: Path) -> List[Path]:
#         """
#         Recursively find all image files in directory.

#         Args:
#             root: Root directory path.

#         Returns:
#             List of image file paths.
#         """
#         files = []
#         for ext in self.image_extensions:
#             files.extend(root.rglob(f"*{ext}"))
#         return files


# def augment_dataset(
#     input_dir: str,
#     output_dir: str,
#     augmentations_per_image: int = 5,
#     seed: int = 42,
# ) -> None:
#     """
#     Backward-compatible wrapper for legacy code.

#     Args:
#         input_dir: Directory containing cleaned images (organized by class).
#         output_dir: Directory to save augmented images.
#         augmentations_per_image: Number of augmented versions per original image.
#         seed: Random seed for reproducibility.
#     """
#     augmenter = ImageAugmenter(
#         augmentations_per_image=augmentations_per_image, seed=seed, save_original=True
#     )
#     augmenter.process_directory(input_dir, output_dir)




# import torch
# import torchvision.transforms as transforms
# from torchvision.transforms import autoaugment, transforms
# import numpy as np
# import random
# import os
# from PIL import Image
# import torchvision.transforms.functional as F

# class ProgressiveLearning:
#     """æ¸è¿›å¼å­¦ä¹ ï¼šä»ç®€å•åˆ°å¤æ‚çš„è®­ç»ƒç­–ç•¥"""
#     def __init__(self, image_sizes=[32, 48, 64, 80], epochs_per_stage=[20, 30, 40, 10]):
#         self.image_sizes = image_sizes
#         self.epochs_per_stage = epochs_per_stage
#         self.current_stage = 0
        
#     def get_current_transform(self, is_train=True):
#         size = self.image_sizes[self.current_stage]
#         if is_train:
#             return transforms.Compose([
#                 transforms.Resize((size + 8, size + 8)),
#                 transforms.RandomCrop(size, padding=4),
#                 transforms.RandomHorizontalFlip(p=0.5),
#                 transforms.RandomRotation(15),
#                 transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
#                 transforms.RandomGrayscale(p=0.1),
#                 transforms.ToTensor(),
#                 transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),
#                 transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3))
#             ])
#         else:
#             return transforms.Compose([
#                 transforms.Resize((size, size)),
#                 transforms.ToTensor(),
#                 transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
#             ])
    
#     def should_advance_stage(self, epoch, total_epochs):
#         cumulative_epochs = sum(self.epochs_per_stage[:self.current_stage+1])
#         return epoch >= cumulative_epochs and self.current_stage < len(self.image_sizes)-1
    
#     def advance_stage(self):
#         self.current_stage += 1
#         print(f"æ¸è¿›å¼å­¦ä¹ : è¿›å…¥é˜¶æ®µ {self.current_stage}, å›¾åƒå°ºå¯¸: {self.image_sizes[self.current_stage]}")

# class CIFAR100AdvancedAugmentation:
#     def __init__(self, progressive_learning=None):
#         self.progressive_learning = progressive_learning
        
#         # æ ‡å‡†å¢å¼ºï¼ˆå¦‚æœä¸ç”¨æ¸è¿›å¼å­¦ä¹ ï¼‰
#         self.standard_train_transform = transforms.Compose([
#             transforms.Resize(72),
#             transforms.RandomCrop(64, padding=4),
#             transforms.RandomHorizontalFlip(p=0.5),
#             transforms.RandomRotation(15),
#             transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
#             transforms.RandomGrayscale(p=0.1),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),
#             transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3))
#         ])
        
#         self.standard_val_transform = transforms.Compose([
#             transforms.Resize(64),
#             transforms.ToTensor(),
#             transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])
#         ])

#     def get_train_transform(self):
#         if self.progressive_learning:
#             return self.progressive_learning.get_current_transform(is_train=True)
#         return self.standard_train_transform

#     def get_val_transform(self):
#         if self.progressive_learning:
#             return self.progressive_learning.get_current_transform(is_train=False)
#         return self.standard_val_transform

# def augment_dataset(input_dir, output_dir, augmentations_per_image=5):
#     """
#     å¯¹æ•°æ®é›†è¿›è¡Œå¢å¼ºï¼Œç”Ÿæˆå¢å¼ºåçš„å›¾åƒå¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•
#     """
#     # åˆ›å»ºè¾“å‡ºç›®å½•
#     os.makedirs(output_dir, exist_ok=True)
    
#     # å®šä¹‰å¢å¼ºå˜æ¢
#     augmentation_transforms = transforms.Compose([
#         transforms.RandomHorizontalFlip(p=0.5),
#         transforms.RandomRotation(degrees=15),
#         transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
#         transforms.RandomResizedCrop(32, scale=(0.8, 1.0), ratio=(0.75, 1.33)),
#         transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
#     ])
    
#     # éå†æ¯ä¸ªç±»åˆ«ç›®å½•
#     for class_name in os.listdir(input_dir):
#         class_input_dir = os.path.join(input_dir, class_name)
#         class_output_dir = os.path.join(output_dir, class_name)
#         os.makedirs(class_output_dir, exist_ok=True)
        
#         # éå†ç±»åˆ«ç›®å½•ä¸­çš„æ¯ä¸ªå›¾åƒ
#         for img_name in os.listdir(class_input_dir):
#             img_path = os.path.join(class_input_dir, img_name)
            
#             # æ‰“å¼€å›¾åƒ
#             try:
#                 image = Image.open(img_path)
#             except Exception as e:
#                 print(f"Error opening image {img_path}: {e}")
#                 continue
            
#             # ä¿å­˜åŸå§‹å›¾åƒ
#             base_name = os.path.splitext(img_name)[0]
#             image.save(os.path.join(class_output_dir, f"{base_name}_0.jpg"))
            
#             # ç”Ÿæˆå¢å¼ºå›¾åƒ
#             for i in range(augmentations_per_image):
#                 augmented_image = augmentation_transforms(image)
#                 augmented_image.save(os.path.join(class_output_dir, f"{base_name}_{i+1}.jpg"))
    
#     print(f"Data augmentation completed. Augmented images saved to {output_dir}")

# def load_transforms(progressive_learning=None):
#     """
#     è¿”å›ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„æ•°æ®å˜æ¢
#     """
#     augmentation = CIFAR100AdvancedAugmentation(progressive_learning)
#     return {
#         'train': augmentation.get_train_transform(),
#         'val': augmentation.get_val_transform()
#     }

import logging
import random
from pathlib import Path
from typing import List, Tuple

import numpy as np
import torch
import albumentations as A
from PIL import Image

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ImageAugmenter:
    """Class to handle image augmentation operations using Albumentations."""

    def __init__(
        self,
        augmentations_per_image: int = 5,
        seed: int = 42,
        save_original: bool = True,
        image_extensions: Tuple[str, ...] = (".png", ".jpg", ".jpeg"),
    ):
        """
        Initialize the ImageAugmenter.

        Args:
            augmentations_per_image: Number of augmented versions per original image.
            seed: Random seed for reproducibility.
            save_original: Whether to save the original image with prefix 'orig_'.
            image_extensions: Tuple of valid image file extensions.
        """
        self.augmentations_per_image = augmentations_per_image
        self.seed = seed
        self.save_original = save_original
        self.image_extensions = image_extensions

        self._set_seed()

        # Define Albumentations pipeline
        self.transform = A.Compose(
            [
                A.Rotate(limit=15, p=0.8),
                A.HorizontalFlip(p=0.5),
                A.ShiftScaleRotate(
                    shift_limit=0.1,
                    scale_limit=0.1,
                    rotate_limit=0,
                    p=0.8,
                    border_mode=0,  # cv2.BORDER_CONSTANT
                ),
                A.ColorJitter(
                    brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.8
                ),
                A.OneOf(
                    [
                        A.GaussianBlur(blur_limit=(3, 7), p=0.5),
                        A.MotionBlur(blur_limit=7, p=0.5),
                    ],
                    p=0.3,
                ),
                A.RandomBrightnessContrast(p=0.2),
            ]
        )

    def _set_seed(self):
        """Set random seeds for reproducibility."""
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(self.seed)

    def augment_image(self, image: Image.Image) -> Image.Image:
        """
        Apply augmentation transforms to a single image using Albumentations.

        Args:
            image: PIL Image to augment.

        Returns:
            Augmented PIL Image.
        """
        # Convert PIL to NumPy array (RGB)
        image_np = np.array(image)

        # Apply Albumentations transform
        augmented = self.transform(image=image_np)
        augmented_image_np = augmented["image"]

        # Convert back to PIL Image
        return Image.fromarray(augmented_image_np.astype(np.uint8))

    def process_directory(self, input_dir: str, output_dir: str) -> None:
        """
        Augment all images in input directory and save to output directory.

        Preserves folder structure. Skips files that fail to load.

        Args:
            input_dir: Path to input directory with class subfolders.
            output_dir: Path to output directory for augmented images.
        """
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        count = 0

        image_files = self._find_image_files(input_path)

        logger.info(f"Found {len(image_files)} images to augment.")

        for img_path in image_files:
            try:
                image = Image.open(img_path).convert("RGB")
            except Exception as e:
                logger.warning(f"Failed to load image {img_path}: {e}")
                continue

            # Determine output subdirectory
            rel_dir = img_path.parent.relative_to(input_path)
            target_dir = output_path / rel_dir
            if not target_dir.exists():
                target_dir.mkdir(parents=True, exist_ok=True)

            # Save original if requested
            if self.save_original:
                orig_name = f"orig_{img_path.name}"
                image.save(target_dir / orig_name)

            # Generate and save augmented versions
            for i in range(self.augmentations_per_image):
                augmented = self.augment_image(image.copy())
                aug_name = f"aug_{i}_{img_path.name}"
                augmented.save(target_dir / aug_name)
                count += 1

        logger.info(
            f"Augmentation of {count} images completed. Output saved to: {output_dir}"
        )

    def _find_image_files(self, root: Path) -> List[Path]:
        """
        Recursively find all image files in directory.

        Args:
            root: Root directory path.

        Returns:
            List of image file paths.
        """
        files = []
        for ext in self.image_extensions:
            files.extend(root.rglob(f"*{ext}"))
        return files


def augment_dataset(
    input_dir: str,
    output_dir: str,
    augmentations_per_image: int = 5,
    seed: int = 42,
) -> None:
    """
    Backward-compatible wrapper for legacy code.

    Args:
        input_dir: Directory containing cleaned images (organized by class).
        output_dir: Directory to save augmented images.
        augmentations_per_image: Number of augmented versions per original image.
        seed: Random seed for reproducibility.
    """
    augmenter = ImageAugmenter(
        augmentations_per_image=augmentations_per_image, seed=seed, save_original=True
    )
    augmenter.process_directory(input_dir, output_dir)